{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEKWoh3p1Mv"
   },
   "source": [
    "# Homework Description\n",
    "- English to Chinese (Traditional) Translation\n",
    "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
    "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
    "\n",
    "- TODO\n",
    "    - Train a simple RNN seq2seq to acheive translation\n",
    "    - Switch to transformer model to boost performance\n",
    "    - Apply Back-translation to furthur boost performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Vf1Q79XPQ3D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  4 13:57:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 35%   31C    P0    67W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:B3:00.0 Off |                  N/A |\n",
      "| 26%   28C    P0    32W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59neB_Sxp5Ub"
   },
   "source": [
    "# Download and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rRlFbfFRpZYT"
   },
   "outputs": [],
   "source": [
    "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
    "# !pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fSksMTdmp-Wt"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pytorch/fairseq.git\n",
    "# !cd fairseq && git checkout 9a1c497\n",
    "# !pip install --upgrade ./fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uRLTiuIuqGNc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from fairseq import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n07Za1XqJzA"
   },
   "source": [
    "# Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xllxxyWxqI7s"
   },
   "outputs": [],
   "source": [
    "seed = 73\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed) \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5ORDJ-2qdYw"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "## En-Zh Bilingual Parallel Corpus\n",
    "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
    "    - Raw: 398,066 (sentences)   \n",
    "    - Processed: 393,980 (sentences)\n",
    "    \n",
    "\n",
    "## Testdata\n",
    "- Size: 4,000 (sentences)\n",
    "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQw2mY4Dqkzd"
   },
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SXT42xQtqijD"
   },
   "outputs": [],
   "source": [
    "data_dir = './DATA/rawdata'\n",
    "dataset_name = 'ted2020'\n",
    "urls = (\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
    "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
    ")\n",
    "file_names = (\n",
    "    'ted2020.tgz', # train & dev\n",
    "    'test.tgz', # test\n",
    ")\n",
    "prefix = Path(data_dir).absolute() / dataset_name\n",
    "\n",
    "# prefix.mkdir(parents=True, exist_ok=True)\n",
    "# for u, f in zip(urls, file_names):\n",
    "#     path = prefix/f\n",
    "#     if not path.exists():\n",
    "#         !wget {u} -O {path}\n",
    "#     if path.suffix == \".tgz\":\n",
    "#         !tar -xvf {path} -C {prefix}\n",
    "#     elif path.suffix == \".zip\":\n",
    "#         !unzip -o {path} -d {prefix}\n",
    "# !mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
    "# !mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
    "# !mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
    "# !mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
    "# !rm -rf {prefix/'test'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLkJwNiFrIwZ"
   },
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_uJYkCncrKJb"
   },
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = f'{prefix}/train_dev.raw'\n",
    "test_prefix = f'{prefix}/test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0t2CPt1brOT3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris.\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "And I say that sincerely, partly because  I need that.\n",
      "Put yourselves in my position.\n",
      "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸。我非常感激。\n",
      "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
      "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
      "請你們設身處地為我想一想！\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRoE9UK7r1gY"
   },
   "source": [
    "## Preprocess files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3tzFwtnFrle3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    \"\"\"Full width -> half width\"\"\"\n",
    "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:  # Full width space: direct conversion\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "                \n",
    "def clean_s(s, lang):\n",
    "    if lang == 'en':\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace('-', '') # remove '-'\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
    "    elif lang == 'zh':\n",
    "        s = strQ2B(s) # Q2B\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "        s = s.replace(' ', '')\n",
    "        s = s.replace('—', '')\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        s = s.replace('_', '')\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
    "    s = ' '.join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def len_s(s, lang):\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())\n",
    "\n",
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
    "                    for s1 in l1_in_f:\n",
    "                        s1 = s1.strip()\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h_i8b1PRr9Nf"
   },
   "outputs": [],
   "source": [
    "# clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "# clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gjT3XCy9r_rj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much , Chris .\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
      "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
      "And I say that sincerely , partly because I need that .\n",
      "Put yourselves in my position .\n",
      "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
      "真是一大榮幸 。 我非常感激 。\n",
      "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
      "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
      "請你們設身處地為我想一想 !\n"
     ]
    }
   ],
   "source": [
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKb4u67-sT_Z"
   },
   "source": [
    "## Split into train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AuFKeDz3sGHL"
   },
   "outputs": [],
   "source": [
    "valid_ratio = 0.01 # 3000~4000 would suffice\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QR2NVldqsXyY"
   },
   "outputs": [],
   "source": [
    "# if (prefix/f'train.clean.{src_lang}').exists() \\\n",
    "# and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
    "# and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
    "# and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
    "#     print(f'train/valid splits exists. skipping split.')\n",
    "# else:\n",
    "#     line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "#     labels = list(range(line_num))\n",
    "#     random.shuffle(labels)\n",
    "#     for lang in [src_lang, tgt_lang]:\n",
    "#         train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "#         valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "#         count = 0\n",
    "#         for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "#             if labels[count]/line_num < train_ratio:\n",
    "#                 train_f.write(line)\n",
    "#             else:\n",
    "#                 valid_f.write(line)\n",
    "#             count += 1\n",
    "#         train_f.close()\n",
    "#         valid_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1rwQysTsdJq"
   },
   "source": [
    "## Subword Units \n",
    "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
    "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
    "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ecwllsa7sZRA"
   },
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# vocab_size = 8000\n",
    "# if (prefix/f'spm{vocab_size}.model').exists():\n",
    "#     print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "# else:\n",
    "#     spm.SentencePieceTrainer.train(\n",
    "#         input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "#                         f'{prefix}/valid.clean.{src_lang}',\n",
    "#                         f'{prefix}/train.clean.{tgt_lang}',\n",
    "#                         f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "#         model_prefix=prefix/f'spm{vocab_size}',\n",
    "#         vocab_size=vocab_size,\n",
    "#         character_coverage=1,\n",
    "#         model_type='unigram', # 'bpe' works as well\n",
    "#         input_sentence_size=1e6,\n",
    "#         shuffle_input_sentence=True,\n",
    "#         normalization_rule_name='nmt_nfkc_cf',\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lQPRNldqse_V"
   },
   "outputs": [],
   "source": [
    "# spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "# in_tag = {\n",
    "#     'train': 'train.clean',\n",
    "#     'valid': 'valid.clean',\n",
    "#     'test': 'test.raw.clean',\n",
    "# }\n",
    "# for split in ['train', 'valid', 'test']:\n",
    "#     for lang in [src_lang, tgt_lang]:\n",
    "#         out_path = prefix/f'{split}.{lang}'\n",
    "#         if out_path.exists():\n",
    "#             print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "#         else:\n",
    "#             with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
    "#                 with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
    "#                     for line in in_f:\n",
    "#                         line = line.strip()\n",
    "#                         tok = spm_model.encode(line, out_type=str)\n",
    "#                         print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4j6lXHjAsjXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
      "▁and ▁it ' s ▁truly ▁a ▁great ▁honor ▁to ▁have ▁the ▁opportunity ▁to ▁come ▁to ▁this ▁stage ▁twice ▁; ▁i ' m ▁extreme ly ▁grateful ▁.\n",
      "▁i ▁have ▁been ▁blow n ▁away ▁by ▁this ▁conference ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁nice ▁comment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
      "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
      "▁put ▁your s el ve s ▁in ▁my ▁position ▁.\n",
      "▁非常謝謝 你 ▁, ▁克里斯 ▁。 ▁能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
      "▁ 真是 一大 榮幸 ▁。 ▁我 非常 感激 ▁。\n",
      "▁這個 研 討 會 給我 留下 了 極為 深刻 的 印象 ▁, ▁我想 感謝 大家 對我 之前 演講 的 好 評 ▁。\n",
      "▁我 是由 衷 的 想 這麼說 ▁, ▁有 部份 原因 是因為我 真的 有 需要 ▁!\n",
      "▁請 你們 設 身處 地 為我 想一想 ▁!\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59si_C0Wsms7"
   },
   "source": [
    "## Binarize the data with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "w-cHVLSpsknh"
   },
   "outputs": [],
   "source": [
    "# binpath = Path('./DATA/data-bin', dataset_name)\n",
    "# if binpath.exists():\n",
    "#     print(binpath, \"exists, will not overwrite!\")\n",
    "# else:\n",
    "#     !python -m fairseq_cli.preprocess \\\n",
    "#         --source-lang {src_lang}\\\n",
    "#         --target-lang {tgt_lang}\\\n",
    "#         --trainpref {prefix/'train'}\\\n",
    "#         --validpref {prefix/'valid'}\\\n",
    "#         --testpref {prefix/'test'}\\\n",
    "#         --destdir {binpath}\\\n",
    "#         --joined-dictionary\\\n",
    "#         --workers 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szMuH1SWLPWA"
   },
   "source": [
    "# Configuration for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5Luz3_tVLUxs"
   },
   "outputs": [],
   "source": [
    "#! config\n",
    "config = Namespace(\n",
    "    datadir = \"DATA/data-bin/ted2020_with_mono\",\n",
    "    savedir = \"./checkpoints/transformer-bt-big\",\n",
    "    run_train = True,\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=10,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2,\n",
    "    lr_warmup=10000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=60,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10, \n",
    "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjrJFvyQLg86"
   },
   "source": [
    "# Logging\n",
    "- logging package logs ordinary messages\n",
    "- wandb logs the loss, bleu, etc. in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-ZiMyDWALbDk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:03 | ERROR | wandb.jupyter | Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtedli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lpz106u/course/ML/hw5/wandb/run-20220404_135804-3vaspwgq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tedli/ml-hw5/runs/3vaspwgq\" target=\"_blank\">transformer-bt-big</a></strong> to <a href=\"https://wandb.ai/tedli/ml-hw5\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"ml-hw5\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNoSkK45Lmqc"
   },
   "source": [
    "# CUDA Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oqrsbmcoLqMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-04-04 13:58:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               \n",
      "2022-04-04 13:58:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
     ]
    }
   ],
   "source": [
    "cuda_env = utils.CudaEnvironment()\n",
    "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJuBIHLLt2D"
   },
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpG4EBRLwe_"
   },
   "source": [
    "## We borrow the TranslationTask from fairseq\n",
    "* used to load the binarized data created above\n",
    "* well-implemented data iterator (dataloader)\n",
    "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
    "* well-implemented beach search decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3gSEy1uFLvVs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:08 | INFO | fairseq.tasks.translation | [en] dictionary: 15992 types\n",
      "2022-04-04 13:58:08 | INFO | fairseq.tasks.translation | [zh] dictionary: 15992 types\n"
     ]
    }
   ],
   "source": [
    "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
    "# from fairseq.tasks import translation\n",
    "## setup task\n",
    "task_cfg = TranslationConfig(\n",
    "    data=config.datadir,\n",
    "    source_lang=config.source_lang,\n",
    "    target_lang=config.target_lang,\n",
    "    train_subset=\"train\",\n",
    "    required_seq_len_multiple=8,\n",
    "    dataset_impl=\"mmap\",\n",
    "    upsample_primary=1,\n",
    ")\n",
    "task = TranslationTask.setup_task(task_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mR7Bhov7L4IU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:08 | INFO | ml-hw5 | loading data for epoch 1\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: DATA/data-bin/ted2020_with_mono/train.zh-en.en\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: DATA/data-bin/ted2020_with_mono/train.zh-en.zh\n",
      "2022-04-04 13:58:08 | INFO | fairseq.tasks.translation | DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 780,952 examples from: DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 780,952 examples from: DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
      "2022-04-04 13:58:08 | INFO | fairseq.tasks.translation | DATA/data-bin/ted2020_with_mono train1 en-zh 780952 examples\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: DATA/data-bin/ted2020_with_mono/valid.zh-en.en\n",
      "2022-04-04 13:58:08 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: DATA/data-bin/ted2020_with_mono/valid.zh-en.zh\n",
      "2022-04-04 13:58:08 | INFO | fairseq.tasks.translation | DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"loading data for epoch 1\")\n",
    "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
    "task.load_dataset(split=\"valid\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "P0BCEm_9L6ig"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1,\n",
      " 'source': tensor([  16,   13,    8, 1423,   44,   18,   62,    6,  510,   23, 1320, 1030,\n",
      "        1217,    5,    2]),\n",
      " 'target': tensor([ 128, 3920, 6217, 3166,  323,  388,  444, 9526, 3855,    2])}\n",
      "\"Source: that's exactly what i do optical mind control .\"\n",
      "'Target: 這實在就是我所做的--光學操控思想'\n"
     ]
    }
   ],
   "source": [
    "sample = task.dataset(\"valid\")[1]\n",
    "pprint.pprint(sample)\n",
    "pprint.pprint(\n",
    "    \"Source: \" + \\\n",
    "    task.source_dictionary.string(\n",
    "        sample['source'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")\n",
    "pprint.pprint(\n",
    "    \"Target: \" + \\\n",
    "    task.target_dictionary.string(\n",
    "        sample['target'],\n",
    "        config.post_process,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcfCVa2FMBSE"
   },
   "source": [
    "# Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBvc-B_6MKZM"
   },
   "source": [
    "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
    "* Shuffles the training set for every epoch\n",
    "* Ignore sentences exceeding maximum length\n",
    "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
    "* Add eos and shift one token\n",
    "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
    "    - generally, prepending bos to the target would do the job (as shown below)\n",
    "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
    "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
    "    ```\n",
    "    # output target (target) and Decoder input (prev_output_tokens): \n",
    "                   eos = 2\n",
    "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
    "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OWFJFmCnMDXW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:08 | WARNING | fairseq.tasks.fairseq_task | 2,079 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 1025, 2524, 682, 935, 1789, 2064, 2490, 3865, 222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': tensor([2640]),\n",
       " 'nsentences': 1,\n",
       " 'ntokens': 6,\n",
       " 'net_input': {'src_tokens': tensor([[  1,   1,   1,   1,   1,   7, 526,  22,  30,  94,  62,  20,  62,  19,\n",
       "            27,   2]]),\n",
       "  'src_lengths': tensor([11]),\n",
       "  'prev_output_tokens': tensor([[    2,  6898,    30,    63, 11204,    27,     1,     1]])},\n",
       " 'target': tensor([[ 6898,    30,    63, 11204,    27,     2,     1,     1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=10, cached=True):\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=task.dataset(split),\n",
    "        max_tokens=max_tokens,\n",
    "        max_sentences=None,\n",
    "        max_positions=utils.resolve_max_positions(\n",
    "            task.max_positions(),\n",
    "            max_tokens,\n",
    "        ),\n",
    "        ignore_invalid_inputs=True,\n",
    "        seed=seed,\n",
    "        num_workers=num_workers,\n",
    "        epoch=epoch,\n",
    "        disable_iterator_cache=not cached,\n",
    "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
    "        # first call of this method has no effect. \n",
    "    )\n",
    "    return batch_iterator\n",
    "\n",
    "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
    "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
    "sample = next(demo_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p86K-0g7Me4M"
   },
   "source": [
    "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
    "```python\n",
    "batch = {\n",
    "    \"id\": id, # id for each example \n",
    "    \"nsentences\": len(samples), # batch size (sentences)\n",
    "    \"ntokens\": ntokens, # batch size (tokens)\n",
    "    \"net_input\": {\n",
    "        \"src_tokens\": src_tokens, # sequence in source language\n",
    "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
    "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
    "    },\n",
    "    \"target\": target, # target sequence\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EyDBE5ZMkFZ"
   },
   "source": [
    "# Model Architecture\n",
    "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Hzh74qLIMfW_"
   },
   "outputs": [],
   "source": [
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    FairseqIncrementalDecoder,\n",
    "    FairseqEncoderDecoderModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI46v1z7MotH"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn0wSeLLMrbc"
   },
   "source": [
    "- The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.\n",
    "- Parameters:\n",
    "  - *args*\n",
    "      - encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction\n",
    "      - encoder_ffn_embed_dim is the dimension of hidden states and output vectors\n",
    "      - encoder_layers is the number of layers for Encoder RNN\n",
    "      - dropout determines the probability of a neuron's activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.\n",
    "  - *dictionary*: the dictionary provided by fairseq. it's used to obtain the padding index, and in turn the encoder padding mask. \n",
    "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
    "\n",
    "- Inputs: \n",
    "    - *src_tokens*: integer sequence representing english e.g. 1, 28, 29, 205, 2 \n",
    "- Outputs: \n",
    "    - *outputs*: the output of RNN at each timestep, can be furthur processed by Attention\n",
    "    - *final_hiddens*: the hidden states of each timestep, will be passed to decoder for decoding\n",
    "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WcX3W4iGMq-S"
   },
   "outputs": [],
   "source": [
    "class RNNEncoder(FairseqEncoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        self.embed_dim = args.encoder_embed_dim\n",
    "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
    "        self.num_layers = args.encoder_layers\n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self.padding_idx = dictionary.pad()\n",
    "        \n",
    "    def combine_bidir(self, outs, bsz: int):\n",
    "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
    "        return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "    def forward(self, src_tokens, **unused):\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # pass thru bidirectional RNN\n",
    "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
    "        x, final_hiddens = self.rnn(x, h0)\n",
    "        outputs = self.dropout_out_module(x)\n",
    "        # outputs = [sequence len, batch size, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        \n",
    "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
    "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
    "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
    "        \n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "        return tuple(\n",
    "            (\n",
    "                outputs,  # seq_len x batch x hidden\n",
    "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
    "                encoder_padding_mask,  # seq_len x batch\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        return tuple(\n",
    "            (\n",
    "                encoder_out[0].index_select(1, new_order),\n",
    "                encoder_out[1].index_select(1, new_order),\n",
    "                encoder_out[2].index_select(1, new_order),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZlE_1JnMv56"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSFSKt_ZMzgh"
   },
   "source": [
    "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
    "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
    "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
    "\n",
    "- Parameters:\n",
    "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
    "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
    "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
    "\n",
    "- Inputs: \n",
    "    - *inputs*: is the key, the vector to attend to others\n",
    "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
    "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
    "- Outputs: \n",
    "    - *output*: the context vector after attention\n",
    "    - *attention score*: the attention distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1Atf_YuCMyyF"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(\n",
    "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
    "        # inputs: T, B, dim\n",
    "        # encoder_outputs: S x B x dim\n",
    "        # padding mask:  S x B\n",
    "        \n",
    "        # convert all to batch first\n",
    "        inputs = inputs.transpose(1,0) # B, T, dim\n",
    "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
    "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
    "        \n",
    "        # project to the dimensionality of encoder_outputs\n",
    "        x = self.input_proj(inputs)\n",
    "\n",
    "        # compute attention\n",
    "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
    "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
    "\n",
    "        # cancel the attention at positions corresponding to padding\n",
    "        if encoder_padding_mask is not None:\n",
    "            # leveraging broadcast  B, S -> (B, 1, S)\n",
    "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
    "            attn_scores = (\n",
    "                attn_scores.float()\n",
    "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
    "                .type_as(attn_scores)\n",
    "            )  # FP16 support: cast to float and back\n",
    "\n",
    "        # softmax on the dimension corresponding to source sequence\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
    "        x = torch.bmm(attn_scores, encoder_outputs)\n",
    "\n",
    "        # (B, T, dim)\n",
    "        x = torch.cat((x, inputs), dim=-1)\n",
    "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
    "        \n",
    "        # restore shape (B, T, dim) -> (T, B, dim)\n",
    "        return x.transpose(1,0), attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doSCOA2gM7fK"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M8Vod2gNABR"
   },
   "source": [
    "* The hidden states of **Decoder** will be initialized by the final hidden states of **Encoder** (the content vector)\n",
    "* At the same time, **Decoder** will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output\n",
    "* Attention improves the performance\n",
    "* The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.\n",
    "- Parameters:\n",
    "  - *args*\n",
    "      - decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，\n",
    "      - decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim\n",
    "      - decoder_layers: number of layers of RNN decoder\n",
    "      - share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings\n",
    "  - *dictionary*: the dictionary provided by fairseq\n",
    "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
    "- Inputs: \n",
    "    - *prev_output_tokens*: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2 \n",
    "    - *encoder_out*: encoder's output.\n",
    "    - *incremental_state*: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.\n",
    "- Outputs: \n",
    "    - *outputs*: the logits (before softmax) output of decoder for each timesteps\n",
    "    - *extra*: unsused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QfvgqHYDM6Lp"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(FairseqIncrementalDecoder):\n",
    "    def __init__(self, args, dictionary, embed_tokens):\n",
    "        super().__init__(dictionary)\n",
    "        self.embed_tokens = embed_tokens\n",
    "        \n",
    "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
    "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
    "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
    "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
    "        \n",
    "        self.embed_dim = args.decoder_embed_dim\n",
    "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
    "        self.num_layers = args.decoder_layers\n",
    "        \n",
    "        \n",
    "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
    "        self.rnn = nn.GRU(\n",
    "            self.embed_dim, \n",
    "            self.hidden_dim, \n",
    "            self.num_layers, \n",
    "            dropout=args.dropout, \n",
    "            batch_first=False, \n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.attention = AttentionLayer(\n",
    "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
    "        ) \n",
    "        # self.attention = None\n",
    "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
    "        \n",
    "        if self.hidden_dim != self.embed_dim:\n",
    "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        else:\n",
    "            self.project_out_dim = None\n",
    "        \n",
    "        if args.share_decoder_input_output_embed:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.embed_tokens.weight.shape[1],\n",
    "                self.embed_tokens.weight.shape[0],\n",
    "                bias=False,\n",
    "            )\n",
    "            self.output_projection.weight = self.embed_tokens.weight\n",
    "        else:\n",
    "            self.output_projection = nn.Linear(\n",
    "                self.output_embed_dim, len(dictionary), bias=False\n",
    "            )\n",
    "            nn.init.normal_(\n",
    "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
    "            )\n",
    "        \n",
    "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
    "        # extract the outputs from encoder\n",
    "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
    "        # outputs:          seq_len x batch x num_directions*hidden\n",
    "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
    "        # padding_mask:     seq_len x batch\n",
    "        \n",
    "        if incremental_state is not None and len(incremental_state) > 0:\n",
    "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        else:\n",
    "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
    "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
    "            prev_hiddens = encoder_hiddens\n",
    "        \n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "        \n",
    "        # embed tokens\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = self.dropout_in_module(x)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "                \n",
    "        # decoder-to-encoder attention\n",
    "        if self.attention is not None:\n",
    "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
    "                        \n",
    "        # pass thru unidirectional RNN\n",
    "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
    "        # outputs = [sequence len, batch size, hid dim]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        x = self.dropout_out_module(x)\n",
    "                \n",
    "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
    "        # we need to do an extra projection)\n",
    "        if self.project_out_dim != None:\n",
    "            x = self.project_out_dim(x)\n",
    "        \n",
    "        # project to vocab size\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(1, 0)\n",
    "        \n",
    "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": final_hiddens,\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        \n",
    "        return x, None\n",
    "    \n",
    "    def reorder_incremental_state(\n",
    "        self,\n",
    "        incremental_state,\n",
    "        new_order,\n",
    "    ):\n",
    "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
    "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
    "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
    "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
    "        cache_state = {\n",
    "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
    "        }\n",
    "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDAPmxjRNEEL"
   },
   "source": [
    "## Seq2Seq\n",
    "- Composed of **Encoder** and **Decoder**\n",
    "- Recieves inputs and pass to **Encoder** \n",
    "- Pass the outputs from **Encoder** to **Decoder**\n",
    "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
    "- Once done decoding, return the **Decoder** outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oRwKdLa0NEU6"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(FairseqEncoderDecoderModel):\n",
    "    def __init__(self, args, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.args = args\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        prev_output_tokens,\n",
    "        return_all_hiddens: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the forward pass for an encoder-decoder model.\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(\n",
    "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "        )\n",
    "        logits, extra = self.decoder(\n",
    "            prev_output_tokens,\n",
    "            encoder_out=encoder_out,\n",
    "            src_lengths=src_lengths,\n",
    "            return_all_hiddens=return_all_hiddens,\n",
    "        )\n",
    "        return logits, extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu3C2JfqNHzk"
   },
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nyI9FOx-NJ2m"
   },
   "outputs": [],
   "source": [
    "# # HINT: transformer architecture\n",
    "from fairseq.models.transformer import (\n",
    "    TransformerEncoder, \n",
    "    TransformerDecoder,\n",
    ")\n",
    "\n",
    "def build_model(args, task):\n",
    "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
    "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
    "\n",
    "    # token embeddings\n",
    "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
    "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
    "    \n",
    "    # encoder decoder\n",
    "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
    "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
    "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
    "\n",
    "    # sequence to sequence model\n",
    "    model = Seq2Seq(args, encoder, decoder)\n",
    "    \n",
    "    # initialization for seq2seq model is important, requires extra handling\n",
    "    def init_params(module):\n",
    "        from fairseq.modules import MultiheadAttention\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, MultiheadAttention):\n",
    "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.RNNBase):\n",
    "            for name, param in module.named_parameters():\n",
    "                if \"weight\" in name or \"bias\" in name:\n",
    "                    param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    # weight initialization\n",
    "    model.apply(init_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce5n4eS7NQNy"
   },
   "source": [
    "## Architecture Related Configuration\n",
    "\n",
    "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Cyn30VoGNT6N"
   },
   "outputs": [],
   "source": [
    "arch_args = Namespace(\n",
    "    encoder_embed_dim=1024,\n",
    "    encoder_ffn_embed_dim=4096,\n",
    "    encoder_layers=6,\n",
    "    decoder_embed_dim=1024,\n",
    "    decoder_ffn_embed_dim=4096,\n",
    "    decoder_layers=6,\n",
    "    share_decoder_input_output_embed=True,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "# HINT: these patches on parameters for Transformer\n",
    "def add_transformer_args(args):\n",
    "    args.encoder_attention_heads=4\n",
    "    args.encoder_normalize_before=True\n",
    "    \n",
    "    args.decoder_attention_heads=4\n",
    "    args.decoder_normalize_before=True\n",
    "    \n",
    "    args.activation_fn=\"relu\"\n",
    "    args.max_source_positions=512\n",
    "    args.max_target_positions=512\n",
    "    \n",
    "    # patches on default parameters for Transformer (those not set above)\n",
    "    from fairseq.models.transformer import base_architecture\n",
    "    base_architecture(arch_args)\n",
    "\n",
    "add_transformer_args(arch_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Nbb76QLCNZZZ"
   },
   "outputs": [],
   "source": [
    "if config.use_wandb:\n",
    "    wandb.config.update(vars(arch_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7ZWfxsCDNatH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:14 | INFO | ml-hw5 | Seq2Seq(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(15992, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(15992, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=1024, out_features=15992, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model(arch_args, task)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHll7GRNNdqc"
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUB9f1WCNgMH"
   },
   "source": [
    "## Loss: Label Smoothing Regularization\n",
    "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
    "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
    "* avoids overfitting\n",
    "\n",
    "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "IgspdJn0NdYF"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
    "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, lprobs, target):\n",
    "        if target.dim() == lprobs.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
    "        # equivalent to summing the log probs of all labels\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "        if self.ignore_index is not None:\n",
    "            pad_mask = target.eq(self.ignore_index)\n",
    "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "        else:\n",
    "            nll_loss = nll_loss.squeeze(-1)\n",
    "            smooth_loss = smooth_loss.squeeze(-1)\n",
    "        if self.reduce:\n",
    "            nll_loss = nll_loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        # when calculating cross-entropy, add the loss of other labels\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "# generally, 0.1 is good enough\n",
    "criterion = LabelSmoothedCrossEntropyCriterion(\n",
    "    smoothing=0.1,\n",
    "    ignore_index=task.target_dictionary.pad(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRalDto2NkJJ"
   },
   "source": [
    "## Optimizer: Adam + lr scheduling\n",
    "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
    "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
    "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sS7tQj1ROBYm"
   },
   "outputs": [],
   "source": [
    "def get_rate(d_model, step_num, warmup_step):\n",
    "    lr = d_model **(-.5) * min(step_num ** (-.5), step_num * warmup_step ** (-1.5))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "J8hoAjHPNkh3"
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "    \n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "        \n",
    "    def multiply_grads(self, c):\n",
    "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.mul_(c)\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFJlkOMONsc6"
   },
   "source": [
    "## Scheduling Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "A135fwPCNrQs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0f33853160>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = NoamOpt(\n",
    "    model_size=arch_args.encoder_embed_dim, \n",
    "    factor=config.lr_factor, \n",
    "    warmup=config.lr_warmup, \n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
    "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
    "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOR0g-cVO5ZO"
   },
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-0ZjbK3O8Iv"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "foal3xM1O404"
   },
   "outputs": [],
   "source": [
    "from fairseq.data import iterators\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def batch_cosine_similarity(a:torch.tensor, b: torch.tensor, eps=1e-6):\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.clamp(a_n, min=eps)\n",
    "    b_norm = b / torch.clamp(b_n, min=eps)\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
    "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
    "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
    "    \n",
    "    stats = {\"loss\": []}\n",
    "    scaler = GradScaler() # automatic mixed precision (amp) \n",
    "    \n",
    "    model.train()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
    "    for samples in progress:\n",
    "        model.zero_grad()\n",
    "        accum_loss = 0\n",
    "        sample_size = 0\n",
    "        # gradient accumulation: update every accum_steps samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            if i == 1:\n",
    "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size_i = sample[\"ntokens\"]\n",
    "            sample_size += sample_size_i\n",
    "            \n",
    "            # mixed precision training\n",
    "            with autocast():\n",
    "                net_output = model.forward(**sample[\"net_input\"])\n",
    "                lprobs = F.log_softmax(net_output[0], -1)            \n",
    "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
    "                \n",
    "                # logging\n",
    "                accum_loss += loss.item()\n",
    "                # back-prop\n",
    "                scaler.scale(loss).backward()                \n",
    "        \n",
    "        scaler.unscale_(optimizer)\n",
    "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
    "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # logging\n",
    "        loss_print = accum_loss/sample_size\n",
    "        stats[\"loss\"].append(loss_print)\n",
    "        progress.set_postfix(loss=loss_print)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_print,\n",
    "                \"train/grad_norm\": gnorm.item(),\n",
    "                \"train/lr\": optimizer.rate(),\n",
    "                \"train/sample_size\": sample_size,\n",
    "            })\n",
    "    pos_emb = model.decoder.embed_positions.weights.cpu().detach()\n",
    "    if config.use_wandb:\n",
    "        wandb.log({\"decoder_positional_embedding\": wandb.Image(batch_cosine_similarity(pos_emb, pos_emb))})\n",
    "\n",
    "    loss_print = np.mean(stats[\"loss\"])\n",
    "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt1lX3DRO_yU"
   },
   "source": [
    "## Validation & Inference\n",
    "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
    "- the procedure is essensially same as training, with the addition of inference step\n",
    "- after validation we can save the model weights\n",
    "\n",
    "Validation loss alone cannot describe the actual performance of the model\n",
    "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
    "- We can also manually examine the hypotheses' quality\n",
    "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "2og80HYQPAKq"
   },
   "outputs": [],
   "source": [
    "# fairseq's beam search generator\n",
    "# given model and input seqeunce, produce translation hypotheses by beam search\n",
    "sequence_generator = task.build_generator([model], config)\n",
    "\n",
    "def decode(toks, dictionary):\n",
    "    # convert from Tensor to human readable sentence\n",
    "    s = dictionary.string(\n",
    "        toks.int().cpu(),\n",
    "        config.post_process,\n",
    "    )\n",
    "    return s if s else \"<unk>\"\n",
    "\n",
    "def inference_step(sample, model):\n",
    "    gen_out = sequence_generator.generate([model], sample)\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(gen_out)):\n",
    "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
    "        srcs.append(decode(\n",
    "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
    "            task.source_dictionary,\n",
    "        ))\n",
    "        hyps.append(decode(\n",
    "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "        refs.append(decode(\n",
    "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
    "            task.target_dictionary,\n",
    "        ))\n",
    "    return srcs, hyps, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "y1o7LeDkPDsd"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sacrebleu\n",
    "\n",
    "def validate(model, task, criterion, log_to_wandb=True):\n",
    "    logger.info('begin validation')\n",
    "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
    "    srcs = []\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "            net_output = model.forward(**sample[\"net_input\"])\n",
    "\n",
    "            lprobs = F.log_softmax(net_output[0], -1)\n",
    "            target = sample[\"target\"]\n",
    "            sample_size = sample[\"ntokens\"]\n",
    "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
    "            progress.set_postfix(valid_loss=loss.item())\n",
    "            stats[\"loss\"].append(loss)\n",
    "            \n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            srcs.extend(s)\n",
    "            hyps.extend(h)\n",
    "            refs.extend(r)\n",
    "            \n",
    "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
    "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
    "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
    "    stats[\"srcs\"] = srcs\n",
    "    stats[\"hyps\"] = hyps\n",
    "    stats[\"refs\"] = refs\n",
    "    \n",
    "    if config.use_wandb and log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"valid/loss\": stats[\"loss\"],\n",
    "            \"valid/bleu\": stats[\"bleu\"].score,\n",
    "        }, commit=False)\n",
    "    \n",
    "    showid = np.random.randint(len(hyps))\n",
    "    logger.info(\"example source: \" + srcs[showid])\n",
    "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
    "    logger.info(\"example reference: \" + refs[showid])\n",
    "    \n",
    "    # show bleu results\n",
    "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
    "    logger.info(stats[\"bleu\"].format())\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sRF6nd4PGEE"
   },
   "source": [
    "# Save and Load Model Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "edBuLlkuPGr9"
   },
   "outputs": [],
   "source": [
    "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
    "    stats = validate(model, task, criterion)\n",
    "    bleu = stats['bleu']\n",
    "    loss = stats['loss']\n",
    "    if save:\n",
    "        # save epoch checkpoints\n",
    "        savedir = Path(config.savedir).absolute()\n",
    "        savedir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        check = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
    "            \"optim\": {\"step\": optimizer._step}\n",
    "        }\n",
    "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
    "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
    "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
    "    \n",
    "        # save epoch samples\n",
    "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
    "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
    "                f.write(f\"{s}\\t{h}\\n\")\n",
    "\n",
    "        # get best valid bleu    \n",
    "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
    "            validate_and_save.best_bleu = bleu.score\n",
    "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
    "            \n",
    "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
    "        if del_file.exists():\n",
    "            del_file.unlink()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def try_load_checkpoint(model, optimizer=None, name=None):\n",
    "    name = name if name else \"checkpoint_last.pt\"\n",
    "    checkpath = Path(config.savedir)/name\n",
    "    if checkpath.exists():\n",
    "        check = torch.load(checkpath)\n",
    "        model.load_state_dict(check[\"model\"])\n",
    "        stats = check[\"stats\"]\n",
    "        step = \"unknown\"\n",
    "        if optimizer != None:\n",
    "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
    "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
    "    else:\n",
    "        logger.info(f\"no checkpoints found at {checkpath}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyIFpibfPJ5u"
   },
   "source": [
    "# Main\n",
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "hu7RZbCUPKQr"
   },
   "outputs": [],
   "source": [
    "model = model.to(device=device)\n",
    "criterion = criterion.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5xxlJxU2PeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:18 | INFO | ml-hw5 | task: TranslationTask\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | encoder: TransformerEncoder\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | decoder: TransformerDecoder\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | optimizer: NoamOpt\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | num. model params: 209,113,088 (num. trained: 209,113,088)\n",
      "2022-04-04 13:58:18 | INFO | ml-hw5 | max tokens per batch = 8192, accumulate steps = 2\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
    "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
    "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
    "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
    "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
    "logger.info(\n",
    "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    )\n",
    ")\n",
    "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MSPRqpQUPfaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:58:18 | INFO | ml-hw5 | no checkpoints found at checkpoints/transformer-bt-big/checkpoint_last.pt!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:17:23 | INFO | ml-hw5 | training loss: 7.1104\n",
      "2022-04-04 14:17:23 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:18:06 | INFO | ml-hw5 | example source: and making buildings for this beauty makes cities better places to live .\n",
      "2022-04-04 14:18:06 | INFO | ml-hw5 | example hypothesis: 提供這個城市的物種 。\n",
      "2022-04-04 14:18:06 | INFO | ml-hw5 | example reference: 為了這種美麗來打造建築物 , 能讓城市成為更棒的居住地 。\n",
      "2022-04-04 14:18:06 | INFO | ml-hw5 | validation loss:\t5.9811\n",
      "2022-04-04 14:18:06 | INFO | ml-hw5 | BLEU = 6.34 34.0/12.5/4.7/2.0 (BP = 0.801 ratio = 0.819 hyp_len = 91552 ref_len = 111811)\n",
      "2022-04-04 14:18:08 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint1.pt\n",
      "2022-04-04 14:18:09 | INFO | ml-hw5 | end of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:37:14 | INFO | ml-hw5 | training loss: 5.1218\n",
      "2022-04-04 14:37:14 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:37:56 | INFO | ml-hw5 | example source: and there's no adaption .\n",
      "2022-04-04 14:37:56 | INFO | ml-hw5 | example hypothesis: 而且沒有適應力 。\n",
      "2022-04-04 14:37:56 | INFO | ml-hw5 | example reference: 他們絲毫沒有適應的機會 。\n",
      "2022-04-04 14:37:56 | INFO | ml-hw5 | validation loss:\t4.5871\n",
      "2022-04-04 14:37:56 | INFO | ml-hw5 | BLEU = 18.94 51.1/26.3/14.1/8.1 (BP = 0.958 ratio = 0.959 hyp_len = 107185 ref_len = 111811)\n",
      "2022-04-04 14:38:01 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint2.pt\n",
      "2022-04-04 14:38:06 | INFO | ml-hw5 | end of epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:57:09 | INFO | ml-hw5 | training loss: 4.1575\n",
      "2022-04-04 14:57:09 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:57:47 | INFO | ml-hw5 | example source: then another thing .\n",
      "2022-04-04 14:57:47 | INFO | ml-hw5 | example hypothesis: 然後另一件事\n",
      "2022-04-04 14:57:47 | INFO | ml-hw5 | example reference: 另外一件事 。\n",
      "2022-04-04 14:57:47 | INFO | ml-hw5 | validation loss:\t4.1411\n",
      "2022-04-04 14:57:47 | INFO | ml-hw5 | BLEU = 21.58 58.5/32.2/18.5/11.2 (BP = 0.863 ratio = 0.871 hyp_len = 97426 ref_len = 111811)\n",
      "2022-04-04 14:57:52 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint3.pt\n",
      "2022-04-04 14:57:58 | INFO | ml-hw5 | end of epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:16:55 | INFO | ml-hw5 | training loss: 3.7880\n",
      "2022-04-04 15:16:55 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:17:35 | INFO | ml-hw5 | example source: google answers is something we started , which is really cool , which lets you for five to 100 dollars , you can type a question in , and then there's a pool of researchers that go out and research it for you , and it's guaranteed and all that , and you can get actually very good answers to things without spending all that time yourself .\n",
      "2022-04-04 15:17:35 | INFO | ml-hw5 | example hypothesis: 谷歌的答案是我們開始的 , 這真的很酷 , 這讓你花了5到100美元 , 你可以輸入一個問題 , 然後有一群研究人員去研究它 , 並保證 , 還有這些 , 還有所有的研究 , 你就可以得到答案 。\n",
      "2022-04-04 15:17:35 | INFO | ml-hw5 | example reference: googleanswers是一項我們已經開始的項目 , 這真的非常的酷這讓你能花5到100美元輸入一個你想問的問題接著將有一大群的研究人員為你研究這個問題 , 且有品質保證你可以不用花費自己太多的時間就得到很好的答案\n",
      "2022-04-04 15:17:35 | INFO | ml-hw5 | validation loss:\t3.9077\n",
      "2022-04-04 15:17:35 | INFO | ml-hw5 | BLEU = 24.36 58.2/32.7/19.3/12.0 (BP = 0.945 ratio = 0.947 hyp_len = 105854 ref_len = 111811)\n",
      "2022-04-04 15:17:40 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint4.pt\n",
      "2022-04-04 15:17:45 | INFO | ml-hw5 | end of epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:36:53 | INFO | ml-hw5 | training loss: 3.6035\n",
      "2022-04-04 15:36:53 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:37:33 | INFO | ml-hw5 | example source: it may absorb light , but it certainly does not reflect character .\n",
      "2022-04-04 15:37:33 | INFO | ml-hw5 | example hypothesis: 它可能會吸收光 , 但它肯定不會反映出人格 。\n",
      "2022-04-04 15:37:33 | INFO | ml-hw5 | example reference: 它可以吸收陽光但它肯定不代表個性翻譯:盧紀睿\n",
      "2022-04-04 15:37:33 | INFO | ml-hw5 | validation loss:\t3.7772\n",
      "2022-04-04 15:37:33 | INFO | ml-hw5 | BLEU = 25.06 59.0/33.7/20.1/12.6 (BP = 0.940 ratio = 0.942 hyp_len = 105324 ref_len = 111811)\n",
      "2022-04-04 15:37:39 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint5.pt\n",
      "2022-04-04 15:37:43 | INFO | ml-hw5 | end of epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:56:52 | INFO | ml-hw5 | training loss: 3.4765\n",
      "2022-04-04 15:56:52 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:57:33 | INFO | ml-hw5 | example source: ab: and what month ?\n",
      "2022-04-04 15:57:33 | INFO | ml-hw5 | example hypothesis: 亞瑟:哪月 ?\n",
      "2022-04-04 15:57:33 | INFO | ml-hw5 | example reference: 哪月呢 ? chris:六月 。\n",
      "2022-04-04 15:57:33 | INFO | ml-hw5 | validation loss:\t3.6837\n",
      "2022-04-04 15:57:33 | INFO | ml-hw5 | BLEU = 26.10 58.8/33.8/20.4/13.1 (BP = 0.967 ratio = 0.968 hyp_len = 108188 ref_len = 111811)\n",
      "2022-04-04 15:57:38 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint6.pt\n",
      "2022-04-04 15:57:43 | INFO | ml-hw5 | end of epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:16:47 | INFO | ml-hw5 | training loss: 3.3785\n",
      "2022-04-04 16:16:47 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:17:27 | INFO | ml-hw5 | example source: i thought they were fun .\n",
      "2022-04-04 16:17:27 | INFO | ml-hw5 | example hypothesis: 我覺得他們很好玩 。\n",
      "2022-04-04 16:17:27 | INFO | ml-hw5 | example reference: 但在推特上直播節目的這個想法 ,\n",
      "2022-04-04 16:17:27 | INFO | ml-hw5 | validation loss:\t3.6062\n",
      "2022-04-04 16:17:27 | INFO | ml-hw5 | BLEU = 26.70 59.7/34.9/21.4/13.8 (BP = 0.953 ratio = 0.954 hyp_len = 106658 ref_len = 111811)\n",
      "2022-04-04 16:17:32 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint7.pt\n",
      "2022-04-04 16:17:38 | INFO | ml-hw5 | end of epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:36:43 | INFO | ml-hw5 | training loss: 3.2506\n",
      "2022-04-04 16:36:43 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:37:23 | INFO | ml-hw5 | example source: but all that said , we have a lot of heavy lifting to still do ahead of us .\n",
      "2022-04-04 16:37:23 | INFO | ml-hw5 | example hypothesis: 但這些都說 , 我們還有很多重舉要領先我們 。\n",
      "2022-04-04 16:37:23 | INFO | ml-hw5 | example reference: 但是 , 儘管這樣 , 我們還有很多重要的事情需要解決 。\n",
      "2022-04-04 16:37:23 | INFO | ml-hw5 | validation loss:\t3.5208\n",
      "2022-04-04 16:37:23 | INFO | ml-hw5 | BLEU = 27.17 60.7/36.0/22.3/14.6 (BP = 0.936 ratio = 0.938 hyp_len = 104853 ref_len = 111811)\n",
      "2022-04-04 16:37:28 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint8.pt\n",
      "2022-04-04 16:37:33 | INFO | ml-hw5 | end of epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:56:40 | INFO | ml-hw5 | training loss: 3.1269\n",
      "2022-04-04 16:56:40 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 16:57:21 | INFO | ml-hw5 | example source: we can't .\n",
      "2022-04-04 16:57:21 | INFO | ml-hw5 | example hypothesis: 我們不能 。\n",
      "2022-04-04 16:57:21 | INFO | ml-hw5 | example reference: 這是行不通的 。\n",
      "2022-04-04 16:57:21 | INFO | ml-hw5 | validation loss:\t3.4562\n",
      "2022-04-04 16:57:21 | INFO | ml-hw5 | BLEU = 28.17 60.0/35.7/22.3/14.7 (BP = 0.973 ratio = 0.973 hyp_len = 108845 ref_len = 111811)\n",
      "2022-04-04 16:57:26 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint9.pt\n",
      "2022-04-04 16:57:31 | INFO | ml-hw5 | end of epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:16:39 | INFO | ml-hw5 | training loss: 3.0265\n",
      "2022-04-04 17:16:39 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:17:18 | INFO | ml-hw5 | example source: for example , new york passed a law that said that the teacher improvement data could not be made available and used in the tenure decision for the teachers .\n",
      "2022-04-04 17:17:18 | INFO | ml-hw5 | example hypothesis: 比如 , 紐約通過了一項法律 , 宣稱老師改善資料無法取得 , 且用在終身決策上 。\n",
      "2022-04-04 17:17:18 | INFO | ml-hw5 | example reference: 例如 , 紐約通過了一項法令:用於提升教學水平的資料不能被拿來做為評估老師的依據 , 決定一位老師是否繼續留任 。\n",
      "2022-04-04 17:17:18 | INFO | ml-hw5 | validation loss:\t3.4174\n",
      "2022-04-04 17:17:18 | INFO | ml-hw5 | BLEU = 28.35 61.1/36.5/22.9/15.2 (BP = 0.956 ratio = 0.956 hyp_len = 106945 ref_len = 111811)\n",
      "2022-04-04 17:17:23 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint10.pt\n",
      "2022-04-04 17:17:28 | INFO | ml-hw5 | end of epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:36:34 | INFO | ml-hw5 | training loss: 2.9415\n",
      "2022-04-04 17:36:34 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:37:15 | INFO | ml-hw5 | example source: now , of course , in a mosh pit it's hard to specify a destination .\n",
      "2022-04-04 17:37:15 | INFO | ml-hw5 | example hypothesis: 當然 , 在摩西的坑洞裡 , 很難確定目的地 。\n",
      "2022-04-04 17:37:15 | INFO | ml-hw5 | example reference: 當然 , 在魔秀舞池裡很難料定目的地 ,\n",
      "2022-04-04 17:37:15 | INFO | ml-hw5 | validation loss:\t3.3866\n",
      "2022-04-04 17:37:15 | INFO | ml-hw5 | BLEU = 29.11 60.0/35.9/22.8/15.2 (BP = 0.991 ratio = 0.991 hyp_len = 110810 ref_len = 111811)\n",
      "2022-04-04 17:37:21 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint11.pt\n",
      "2022-04-04 17:37:25 | INFO | ml-hw5 | end of epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:56:27 | INFO | ml-hw5 | training loss: 2.8687\n",
      "2022-04-04 17:56:27 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 17:57:07 | INFO | ml-hw5 | example source: it's a kind of an aria , i would say , and in many tapes that i have .\n",
      "2022-04-04 17:57:07 | INFO | ml-hw5 | example hypothesis: 我會說 , 這是一種詠嘆調 , 在很多我有的錄音帶中 ,\n",
      "2022-04-04 17:57:07 | INFO | ml-hw5 | example reference: 這有點像是獨白 , 在很多我做的錄音訪談裡都有獨白 。\n",
      "2022-04-04 17:57:07 | INFO | ml-hw5 | validation loss:\t3.3563\n",
      "2022-04-04 17:57:07 | INFO | ml-hw5 | BLEU = 29.24 60.8/36.7/23.3/15.7 (BP = 0.973 ratio = 0.974 hyp_len = 108853 ref_len = 111811)\n",
      "2022-04-04 17:57:12 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint12.pt\n",
      "2022-04-04 17:57:17 | INFO | ml-hw5 | end of epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:16:23 | INFO | ml-hw5 | training loss: 2.8028\n",
      "2022-04-04 18:16:23 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:17:02 | INFO | ml-hw5 | example source: intense feelings of imposterism can prevent people from sharing their great ideas or applying for jobs and programs where they’d excel .\n",
      "2022-04-04 18:17:02 | INFO | ml-hw5 | example hypothesis: 強烈的冒名頂替感 , 可以防止人們分享他們偉大的想法 , 或申請工作和專案 , 讓他們被excel排擠 。\n",
      "2022-04-04 18:17:02 | INFO | ml-hw5 | example reference: 強烈的冒名頂替感會讓人無法分享他們很棒的點子或無法應徵那些他們明明可以做得很出色的工作或專案 。\n",
      "2022-04-04 18:17:02 | INFO | ml-hw5 | validation loss:\t3.3289\n",
      "2022-04-04 18:17:02 | INFO | ml-hw5 | BLEU = 29.66 61.3/37.2/23.8/16.1 (BP = 0.970 ratio = 0.971 hyp_len = 108546 ref_len = 111811)\n",
      "2022-04-04 18:17:08 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint13.pt\n",
      "2022-04-04 18:17:13 | INFO | ml-hw5 | end of epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:36:18 | INFO | ml-hw5 | training loss: 2.7450\n",
      "2022-04-04 18:36:18 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:36:57 | INFO | ml-hw5 | example source: everything from health , nutrition , employment all of these are positively impacted when girls are educated .\n",
      "2022-04-04 18:36:57 | INFO | ml-hw5 | example hypothesis: 從健康、營養、就業這些都在女孩受教育時正面影響\n",
      "2022-04-04 18:36:57 | INFO | ml-hw5 | example reference: 從健康、營養、就業當女孩們能接受教育 , 對所有這些都有正面的影響 。\n",
      "2022-04-04 18:36:57 | INFO | ml-hw5 | validation loss:\t3.3153\n",
      "2022-04-04 18:36:57 | INFO | ml-hw5 | BLEU = 30.13 61.4/37.5/24.2/16.5 (BP = 0.973 ratio = 0.973 hyp_len = 108800 ref_len = 111811)\n",
      "2022-04-04 18:37:02 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint14.pt\n",
      "2022-04-04 18:37:07 | INFO | ml-hw5 | end of epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:56:10 | INFO | ml-hw5 | training loss: 2.6943\n",
      "2022-04-04 18:56:10 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:56:48 | INFO | ml-hw5 | example source: worse still , it turned out the bridge’s cable contractor had been selling them faulty wires .\n",
      "2022-04-04 18:56:48 | INFO | ml-hw5 | example hypothesis: 更糟的是 , 這座橋的纜繩承包商賣的是錯誤的電線 。\n",
      "2022-04-04 18:56:48 | INFO | ml-hw5 | example reference: 更糟糕的是 , 結果發現該橋的纜繩包商一直在賣有問題的纜線給他們 。\n",
      "2022-04-04 18:56:48 | INFO | ml-hw5 | validation loss:\t3.2978\n",
      "2022-04-04 18:56:48 | INFO | ml-hw5 | BLEU = 29.80 61.8/37.7/24.4/16.6 (BP = 0.957 ratio = 0.958 hyp_len = 107082 ref_len = 111811)\n",
      "2022-04-04 18:56:53 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint15.pt\n",
      "2022-04-04 18:56:54 | INFO | ml-hw5 | end of epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:15:55 | INFO | ml-hw5 | training loss: 2.6470\n",
      "2022-04-04 19:15:55 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:16:34 | INFO | ml-hw5 | example source: and you see agenda this is marketing , this is politics trying to convince you to have one model versus another , trying to convince you to ignore a model and trust your feelings , marginalizing people with models you don't like .\n",
      "2022-04-04 19:16:34 | INFO | ml-hw5 | example hypothesis: 你看 , 議程──這是行銷 , 政治試著說服你去有一個模型和另一個模型 , 試圖說服你去忽略一個模型 , 並相信你的感覺 , 用你不喜歡的模型邊緣化人群 。\n",
      "2022-04-04 19:16:34 | INFO | ml-hw5 | example reference: 這個規畫表是一種行銷 , 也是政治它企圖影響你信任某種模型而放棄另一個企圖影響去忽視模型只信任你的感覺並且邊緣化那些採用你不喜歡的模型的人\n",
      "2022-04-04 19:16:34 | INFO | ml-hw5 | validation loss:\t3.2915\n",
      "2022-04-04 19:16:34 | INFO | ml-hw5 | BLEU = 30.55 61.3/37.6/24.5/16.8 (BP = 0.979 ratio = 0.979 hyp_len = 109471 ref_len = 111811)\n",
      "2022-04-04 19:16:39 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint16.pt\n",
      "2022-04-04 19:16:44 | INFO | ml-hw5 | end of epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:35:42 | INFO | ml-hw5 | training loss: 2.6049\n",
      "2022-04-04 19:35:42 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:36:21 | INFO | ml-hw5 | example source: you also have this idea for bicycles .\n",
      "2022-04-04 19:36:21 | INFO | ml-hw5 | example hypothesis: 你同時也有自行車的主意 。\n",
      "2022-04-04 19:36:21 | INFO | ml-hw5 | example reference: 你對自行車也有這樣的想法 。\n",
      "2022-04-04 19:36:21 | INFO | ml-hw5 | validation loss:\t3.2770\n",
      "2022-04-04 19:36:21 | INFO | ml-hw5 | BLEU = 30.61 61.7/37.9/24.8/17.1 (BP = 0.970 ratio = 0.970 hyp_len = 108496 ref_len = 111811)\n",
      "2022-04-04 19:36:26 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint17.pt\n",
      "2022-04-04 19:36:31 | INFO | ml-hw5 | end of epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:55:27 | INFO | ml-hw5 | training loss: 2.5660\n",
      "2022-04-04 19:55:27 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:56:07 | INFO | ml-hw5 | example source: let me tell you who i am .\n",
      "2022-04-04 19:56:07 | INFO | ml-hw5 | example hypothesis: 讓我告訴你我是誰 。\n",
      "2022-04-04 19:56:07 | INFO | ml-hw5 | example reference: 讓我向你們介紹我是誰\n",
      "2022-04-04 19:56:07 | INFO | ml-hw5 | validation loss:\t3.2684\n",
      "2022-04-04 19:56:07 | INFO | ml-hw5 | BLEU = 30.42 61.9/38.1/24.9/17.1 (BP = 0.962 ratio = 0.962 hyp_len = 107596 ref_len = 111811)\n",
      "2022-04-04 19:56:12 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint18.pt\n",
      "2022-04-04 19:56:12 | INFO | ml-hw5 | end of epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:15:10 | INFO | ml-hw5 | training loss: 2.5323\n",
      "2022-04-04 20:15:10 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:15:51 | INFO | ml-hw5 | example source: so , in a very real sense , i believe that we , as an international community , should get organized to complete the task .\n",
      "2022-04-04 20:15:51 | INFO | ml-hw5 | example hypothesis: 所以 , 在真實意義上 , 我相信我們身為國際社會 , 應該要有組織來完成這項任務 。\n",
      "2022-04-04 20:15:51 | INFO | ml-hw5 | example reference: 所以 , 我非常認真地相信 , 作為一個國際機構 , 我們應該有組織地完成這個任務 。\n",
      "2022-04-04 20:15:51 | INFO | ml-hw5 | validation loss:\t3.2541\n",
      "2022-04-04 20:15:51 | INFO | ml-hw5 | BLEU = 31.06 61.3/37.8/24.8/17.2 (BP = 0.985 ratio = 0.985 hyp_len = 110166 ref_len = 111811)\n",
      "2022-04-04 20:15:56 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint19.pt\n",
      "2022-04-04 20:16:01 | INFO | ml-hw5 | end of epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:35:02 | INFO | ml-hw5 | training loss: 2.4986\n",
      "2022-04-04 20:35:02 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:35:41 | INFO | ml-hw5 | example source: she knows there's always going to be some part of gator magraw in here , and she wants our daughter and twin sons to discover themselves the way that i did , but probably with fewer body slams and steel chair shots to the head .\n",
      "2022-04-04 20:35:41 | INFO | ml-hw5 | example hypothesis: 她知道總會有一些鱷魚麥克羅的一部分在裡面 , 她希望我們的女兒和雙胞胎兒子用我的方式發現自己 , 但可能少一些身體會撞擊和鋼鐵椅直接衝向頭部 。\n",
      "2022-04-04 20:35:41 | INFO | ml-hw5 | example reference: 她知道永遠都會有一部份的鱷魚麥克羅在這裡 , 她想要我們的女兒和雙胞胎兒子自己去發現 , 像我當初的方式 , 但可能少一些背摔和鐵椅砸頭的攻擊 。\n",
      "2022-04-04 20:35:41 | INFO | ml-hw5 | validation loss:\t3.2489\n",
      "2022-04-04 20:35:41 | INFO | ml-hw5 | BLEU = 31.14 61.6/38.1/25.1/17.4 (BP = 0.979 ratio = 0.979 hyp_len = 109436 ref_len = 111811)\n",
      "2022-04-04 20:35:46 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint20.pt\n",
      "2022-04-04 20:35:51 | INFO | ml-hw5 | end of epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:54:49 | INFO | ml-hw5 | training loss: 2.4696\n",
      "2022-04-04 20:54:49 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 20:55:29 | INFO | ml-hw5 | example source: ca: leymah , thank you . thank you so much for coming to ted .\n",
      "2022-04-04 20:55:29 | INFO | ml-hw5 | example hypothesis: 克里斯:蕾曼 , 謝謝你 。 非常謝謝你來ted 。\n",
      "2022-04-04 20:55:29 | INFO | ml-hw5 | example reference: 克里斯:蕾曼 , 謝謝妳 , 非常感謝妳來到ted\n",
      "2022-04-04 20:55:29 | INFO | ml-hw5 | validation loss:\t3.2445\n",
      "2022-04-04 20:55:29 | INFO | ml-hw5 | BLEU = 31.22 61.3/38.0/25.1/17.5 (BP = 0.982 ratio = 0.982 hyp_len = 109778 ref_len = 111811)\n",
      "2022-04-04 20:55:34 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint21.pt\n",
      "2022-04-04 20:55:39 | INFO | ml-hw5 | end of epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:14:37 | INFO | ml-hw5 | training loss: 2.4409\n",
      "2022-04-04 21:14:37 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:15:16 | INFO | ml-hw5 | example source: but now i understand that it is a record of the beginning .\n",
      "2022-04-04 21:15:16 | INFO | ml-hw5 | example hypothesis: 但現在我明白這是開頭的記錄 。\n",
      "2022-04-04 21:15:16 | INFO | ml-hw5 | example reference: 但現在我相信這是記錄一種開始 。\n",
      "2022-04-04 21:15:16 | INFO | ml-hw5 | validation loss:\t3.2284\n",
      "2022-04-04 21:15:16 | INFO | ml-hw5 | BLEU = 31.50 61.6/38.3/25.4/17.7 (BP = 0.981 ratio = 0.981 hyp_len = 109736 ref_len = 111811)\n",
      "2022-04-04 21:15:21 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint22.pt\n",
      "2022-04-04 21:15:26 | INFO | ml-hw5 | end of epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:34:30 | INFO | ml-hw5 | training loss: 2.4157\n",
      "2022-04-04 21:34:30 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:35:10 | INFO | ml-hw5 | example source: and so that's what i'm trying to do with my work , is to take these numbers , these statistics from the raw language of data , and to translate them into a more universal visual language , that can be felt .\n",
      "2022-04-04 21:35:10 | INFO | ml-hw5 | example hypothesis: 所以 , 那就是我努力在做的 , 是用這些數字來表示 , 這些從資料原始語言取得的統計數字 , 再將它們翻譯成一種更普遍的視覺語言 , 可以被感受到 。\n",
      "2022-04-04 21:35:10 | INFO | ml-hw5 | example reference: 所以我想嘗試用我的作品 , 把這些數字、這些統計數據 , 從原始的資料型態 , 轉化為一般人可以感受到的視覺語言 。\n",
      "2022-04-04 21:35:10 | INFO | ml-hw5 | validation loss:\t3.2296\n",
      "2022-04-04 21:35:10 | INFO | ml-hw5 | BLEU = 31.60 62.0/38.6/25.6/18.0 (BP = 0.976 ratio = 0.976 hyp_len = 109141 ref_len = 111811)\n",
      "2022-04-04 21:35:15 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint23.pt\n",
      "2022-04-04 21:35:20 | INFO | ml-hw5 | end of epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:54:22 | INFO | ml-hw5 | training loss: 2.3918\n",
      "2022-04-04 21:54:22 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 21:55:03 | INFO | ml-hw5 | example source: it was striking to me because it featured a young black girl , probably around 12 years old , looking studiously at some physics equations .\n",
      "2022-04-04 21:55:03 | INFO | ml-hw5 | example hypothesis: 令我驚訝的是 , 它有一個年輕黑人女孩 , 大概12歲了 , 還在不間斷地看一些物理方程式 。\n",
      "2022-04-04 21:55:03 | INFO | ml-hw5 | example reference: 它深深打動了我 , 因為上面是一個年輕黑人女孩 , 大概只有12歲左右 , 孜孜不倦地看著一些物理方程式 。\n",
      "2022-04-04 21:55:03 | INFO | ml-hw5 | validation loss:\t3.2228\n",
      "2022-04-04 21:55:03 | INFO | ml-hw5 | BLEU = 31.80 61.7/38.5/25.6/18.0 (BP = 0.982 ratio = 0.983 hyp_len = 109858 ref_len = 111811)\n",
      "2022-04-04 21:55:08 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint24.pt\n",
      "2022-04-04 21:55:13 | INFO | ml-hw5 | end of epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:14:14 | INFO | ml-hw5 | training loss: 2.3688\n",
      "2022-04-04 22:14:14 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:14:54 | INFO | ml-hw5 | example source: as of next week , it will soon be available , there will be this little bloodpressure meter connected to an iphone or something or other .\n",
      "2022-04-04 22:14:54 | INFO | ml-hw5 | example hypothesis: 下週 , 很快就會有了 , 會有這個小小的血壓計和iphone或其他連結 。\n",
      "2022-04-04 22:14:54 | INFO | ml-hw5 | example reference: 下周即將上市的是這個小的血壓計 , 可以連接到iphone或者其它設備 ,\n",
      "2022-04-04 22:14:54 | INFO | ml-hw5 | validation loss:\t3.2205\n",
      "2022-04-04 22:14:54 | INFO | ml-hw5 | BLEU = 31.99 61.8/38.6/25.8/18.3 (BP = 0.982 ratio = 0.982 hyp_len = 109765 ref_len = 111811)\n",
      "2022-04-04 22:14:59 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint25.pt\n",
      "2022-04-04 22:15:04 | INFO | ml-hw5 | end of epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:34:13 | INFO | ml-hw5 | training loss: 2.3484\n",
      "2022-04-04 22:34:13 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:34:53 | INFO | ml-hw5 | example source: it was pretty hard .\n",
      "2022-04-04 22:34:53 | INFO | ml-hw5 | example hypothesis: 相當困難 。\n",
      "2022-04-04 22:34:53 | INFO | ml-hw5 | example reference: 這的確非常的困難\n",
      "2022-04-04 22:34:53 | INFO | ml-hw5 | validation loss:\t3.2214\n",
      "2022-04-04 22:34:53 | INFO | ml-hw5 | BLEU = 32.23 62.1/39.0/26.1/18.5 (BP = 0.981 ratio = 0.981 hyp_len = 109678 ref_len = 111811)\n",
      "2022-04-04 22:34:58 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint26.pt\n",
      "2022-04-04 22:35:03 | INFO | ml-hw5 | end of epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:54:08 | INFO | ml-hw5 | training loss: 2.3279\n",
      "2022-04-04 22:54:08 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 22:54:47 | INFO | ml-hw5 | example source: so first we will use philanthropy dollars to go into a state to pilot the program and get data .\n",
      "2022-04-04 22:54:47 | INFO | ml-hw5 | example hypothesis: 所以首先我們會用慈善資金進入州內 , 來測試計畫並取得資料 。\n",
      "2022-04-04 22:54:47 | INFO | ml-hw5 | example reference: 因此我們將先用慈善資金啟動州政府進行試點計劃以及收取數據 。\n",
      "2022-04-04 22:54:47 | INFO | ml-hw5 | validation loss:\t3.2146\n",
      "2022-04-04 22:54:47 | INFO | ml-hw5 | BLEU = 32.13 62.5/39.4/26.5/18.8 (BP = 0.965 ratio = 0.966 hyp_len = 107993 ref_len = 111811)\n",
      "2022-04-04 22:54:52 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint27.pt\n",
      "2022-04-04 22:54:52 | INFO | ml-hw5 | end of epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:13:55 | INFO | ml-hw5 | training loss: 2.3096\n",
      "2022-04-04 23:13:55 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:14:34 | INFO | ml-hw5 | example source: more underrepresented characters in terms of race and ethnicity , and most importantly , more women working behind the camera in key production roles .\n",
      "2022-04-04 23:14:34 | INFO | ml-hw5 | example hypothesis: 就種族和人種而言 , 更多代表性不足的角色 , 最重要的是 , 更多女性在相機背後擔任關鍵生產角色 。\n",
      "2022-04-04 23:14:34 | INFO | ml-hw5 | example reference: 詮釋更多代表性不足的少數族群 , 最重要的是讓更多的女性在相機後面擔任關鍵性的製作角色 。\n",
      "2022-04-04 23:14:34 | INFO | ml-hw5 | validation loss:\t3.1997\n",
      "2022-04-04 23:14:34 | INFO | ml-hw5 | BLEU = 32.37 62.4/39.4/26.6/19.0 (BP = 0.969 ratio = 0.970 hyp_len = 108420 ref_len = 111811)\n",
      "2022-04-04 23:14:39 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint28.pt\n",
      "2022-04-04 23:14:44 | INFO | ml-hw5 | end of epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:33:51 | INFO | ml-hw5 | training loss: 2.2913\n",
      "2022-04-04 23:33:51 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:34:31 | INFO | ml-hw5 | example source: but first i have to ask: what are these building blocks , like the alphabet , elements that i showed you ?\n",
      "2022-04-04 23:34:31 | INFO | ml-hw5 | example hypothesis: 但首先我必須問:這些組成元素像字母和顯示的元素是什麼 ?\n",
      "2022-04-04 23:34:31 | INFO | ml-hw5 | example reference: 但首先我有個問題:這些組成的基本單位是什麼 ? 就像我剛給你們看的字母 。\n",
      "2022-04-04 23:34:31 | INFO | ml-hw5 | validation loss:\t3.1967\n",
      "2022-04-04 23:34:31 | INFO | ml-hw5 | BLEU = 32.08 62.1/39.1/26.2/18.7 (BP = 0.972 ratio = 0.972 hyp_len = 108694 ref_len = 111811)\n",
      "2022-04-04 23:34:36 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint29.pt\n",
      "2022-04-04 23:34:36 | INFO | ml-hw5 | end of epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:53:43 | INFO | ml-hw5 | training loss: 2.2746\n",
      "2022-04-04 23:53:43 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-04 23:54:21 | INFO | ml-hw5 | example source: and i didn't hear any of them talking about themselves as if they had been reduced to a thing , totally subjected to the will of the other .\n",
      "2022-04-04 23:54:21 | INFO | ml-hw5 | example hypothesis: 我沒聽過他們怎麼談論自己 , 說得好像他們被簡化成一樣 , 完全遵從別人的意志 。\n",
      "2022-04-04 23:54:21 | INFO | ml-hw5 | example reference: 沒聽到任何一個在談論自己的時候 , 像是已經被降格成一樣東西 , 完全屈服於另一方的意圖 。\n",
      "2022-04-04 23:54:21 | INFO | ml-hw5 | validation loss:\t3.1981\n",
      "2022-04-04 23:54:21 | INFO | ml-hw5 | BLEU = 32.85 62.5/39.7/26.9/19.3 (BP = 0.975 ratio = 0.975 hyp_len = 109009 ref_len = 111811)\n",
      "2022-04-04 23:54:27 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint30.pt\n",
      "2022-04-04 23:54:31 | INFO | ml-hw5 | end of epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:13:37 | INFO | ml-hw5 | training loss: 2.2596\n",
      "2022-04-05 00:13:37 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:14:17 | INFO | ml-hw5 | example source: but many die .\n",
      "2022-04-05 00:14:17 | INFO | ml-hw5 | example hypothesis: 但許多人因此死去 ,\n",
      "2022-04-05 00:14:17 | INFO | ml-hw5 | example reference: 然而有很多人因此死了\n",
      "2022-04-05 00:14:17 | INFO | ml-hw5 | validation loss:\t3.1882\n",
      "2022-04-05 00:14:17 | INFO | ml-hw5 | BLEU = 32.60 62.0/39.2/26.5/19.0 (BP = 0.980 ratio = 0.980 hyp_len = 109614 ref_len = 111811)\n",
      "2022-04-05 00:14:22 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint31.pt\n",
      "2022-04-05 00:14:22 | INFO | ml-hw5 | end of epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:33:25 | INFO | ml-hw5 | training loss: 2.2442\n",
      "2022-04-05 00:33:25 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:34:04 | INFO | ml-hw5 | example source: and when i asked her , \" why is your name 'angry' ? \"\n",
      "2022-04-05 00:34:04 | INFO | ml-hw5 | example hypothesis: 當我問她: 「 為什麼你的名字是『阿基里』 ? 」\n",
      "2022-04-05 00:34:04 | INFO | ml-hw5 | example reference: 我問她: 「 為什麼妳被取名為『憤怒』 」 ?\n",
      "2022-04-05 00:34:04 | INFO | ml-hw5 | validation loss:\t3.1871\n",
      "2022-04-05 00:34:04 | INFO | ml-hw5 | BLEU = 32.69 62.3/39.5/26.7/19.3 (BP = 0.974 ratio = 0.974 hyp_len = 108934 ref_len = 111811)\n",
      "2022-04-05 00:34:09 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint32.pt\n",
      "2022-04-05 00:34:09 | INFO | ml-hw5 | end of epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:53:09 | INFO | ml-hw5 | training loss: 2.2301\n",
      "2022-04-05 00:53:09 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 00:53:50 | INFO | ml-hw5 | example source: and then i tell the computer where the air goes in and out of the cabin , throw in a bunch of physics and basically sit there and wait until the computer calculates the simulation .\n",
      "2022-04-05 00:53:50 | INFO | ml-hw5 | example hypothesis: 然後我告訴電腦 , 空氣進出出出出出艙外 , 進入一堆物理領域 , 然後坐在那裡 , 等電腦計算模擬出來 。\n",
      "2022-04-05 00:53:50 | INFO | ml-hw5 | example reference: 然後我告訴電腦 , 機艙內氣體流動的狀況 , 運用一些物理學知識 , 然後只要坐在一邊 , 等電腦模擬出結果就可以了 。\n",
      "2022-04-05 00:53:50 | INFO | ml-hw5 | validation loss:\t3.1722\n",
      "2022-04-05 00:53:50 | INFO | ml-hw5 | BLEU = 33.04 61.9/39.4/26.8/19.3 (BP = 0.986 ratio = 0.986 hyp_len = 110265 ref_len = 111811)\n",
      "2022-04-05 00:53:55 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint33.pt\n",
      "2022-04-05 00:54:00 | INFO | ml-hw5 | end of epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:13:00 | INFO | ml-hw5 | training loss: 2.2161\n",
      "2022-04-05 01:13:00 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:13:40 | INFO | ml-hw5 | example source: it's complex , and a lot of the terms are quite technical , so the average person probably doesn't need to know at least a third of it .\n",
      "2022-04-05 01:13:40 | INFO | ml-hw5 | example hypothesis: 這很複雜 , 很多的術語都是技術性的 , 所以一般人應該不需要知道至少三分之一 。\n",
      "2022-04-05 01:13:40 | INFO | ml-hw5 | example reference: 它很複雜 , 其中有很多專業詞彙 。 對一般人來說 , 有大概三分之一的詞彙都不用認識 。\n",
      "2022-04-05 01:13:40 | INFO | ml-hw5 | validation loss:\t3.1752\n",
      "2022-04-05 01:13:40 | INFO | ml-hw5 | BLEU = 33.08 61.8/39.2/26.6/19.2 (BP = 0.993 ratio = 0.993 hyp_len = 110977 ref_len = 111811)\n",
      "2022-04-05 01:13:46 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint34.pt\n",
      "2022-04-05 01:13:50 | INFO | ml-hw5 | end of epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:32:49 | INFO | ml-hw5 | training loss: 2.2040\n",
      "2022-04-05 01:32:49 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:33:29 | INFO | ml-hw5 | example source: twentyfour hours a day , this thing was running , mainly running bomb calculations .\n",
      "2022-04-05 01:33:29 | INFO | ml-hw5 | example hypothesis: 一天24小時 , 這傢伙要跑了 , 主要是進行炸彈的計算 。\n",
      "2022-04-05 01:33:29 | INFO | ml-hw5 | example reference: 這台機器一天24小時不停的運轉 , 主要是進行核彈相關的運算\n",
      "2022-04-05 01:33:29 | INFO | ml-hw5 | validation loss:\t3.1757\n",
      "2022-04-05 01:33:29 | INFO | ml-hw5 | BLEU = 33.12 62.0/39.4/26.8/19.3 (BP = 0.987 ratio = 0.987 hyp_len = 110386 ref_len = 111811)\n",
      "2022-04-05 01:33:34 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint35.pt\n",
      "2022-04-05 01:33:39 | INFO | ml-hw5 | end of epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:52:39 | INFO | ml-hw5 | training loss: 2.1916\n",
      "2022-04-05 01:52:39 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 01:53:18 | INFO | ml-hw5 | example source: that because the overdoses are happening so much to the white community .\n",
      "2022-04-05 01:53:18 | INFO | ml-hw5 | example hypothesis: 因為用藥過量對白人社區的影響太大\n",
      "2022-04-05 01:53:18 | INFO | ml-hw5 | example reference: 因為在白人社區發生這麼多用藥過量 。\n",
      "2022-04-05 01:53:18 | INFO | ml-hw5 | validation loss:\t3.1764\n",
      "2022-04-05 01:53:18 | INFO | ml-hw5 | BLEU = 33.13 62.6/40.0/27.3/19.8 (BP = 0.972 ratio = 0.973 hyp_len = 108746 ref_len = 111811)\n",
      "2022-04-05 01:53:23 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint36.pt\n",
      "2022-04-05 01:53:28 | INFO | ml-hw5 | end of epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:12:28 | INFO | ml-hw5 | training loss: 2.1793\n",
      "2022-04-05 02:12:28 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:13:08 | INFO | ml-hw5 | example source: this is nine of them .\n",
      "2022-04-05 02:13:08 | INFO | ml-hw5 | example hypothesis: 這是其中九個\n",
      "2022-04-05 02:13:08 | INFO | ml-hw5 | example reference: 其中有9個\n",
      "2022-04-05 02:13:08 | INFO | ml-hw5 | validation loss:\t3.1644\n",
      "2022-04-05 02:13:08 | INFO | ml-hw5 | BLEU = 33.29 62.3/39.7/27.1/19.7 (BP = 0.981 ratio = 0.982 hyp_len = 109751 ref_len = 111811)\n",
      "2022-04-05 02:13:13 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint37.pt\n",
      "2022-04-05 02:13:18 | INFO | ml-hw5 | end of epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:32:16 | INFO | ml-hw5 | training loss: 2.1683\n",
      "2022-04-05 02:32:16 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:32:54 | INFO | ml-hw5 | example source: but designers , innovators and entrepreneurs , it's our job to not just notice those things , but to go one step further and try to fix them .\n",
      "2022-04-05 02:32:54 | INFO | ml-hw5 | example hypothesis: 但設計師、創新者和企業家 , 我們的工作不只是注意到這些東西 , 還要再進一步 , 試著修復它們 。\n",
      "2022-04-05 02:32:54 | INFO | ml-hw5 | example reference: 但是設計師、革新者、企業家 , 我們的工作不只是注意到這些東西 , 還要再邁一步 , 試著解決這些問題 。\n",
      "2022-04-05 02:32:54 | INFO | ml-hw5 | validation loss:\t3.1569\n",
      "2022-04-05 02:32:54 | INFO | ml-hw5 | BLEU = 33.15 62.6/40.0/27.3/19.8 (BP = 0.972 ratio = 0.972 hyp_len = 108732 ref_len = 111811)\n",
      "2022-04-05 02:33:00 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint38.pt\n",
      "2022-04-05 02:33:00 | INFO | ml-hw5 | end of epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:51:59 | INFO | ml-hw5 | training loss: 2.1572\n",
      "2022-04-05 02:51:59 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 02:52:39 | INFO | ml-hw5 | example source: and they migrated into the rest of the world at two kilometers per year until , within several tens of thousands of years , we occupied every single watershed on the planet and became the most dominant species , with a very small amount of technology .\n",
      "2022-04-05 02:52:39 | INFO | ml-hw5 | example hypothesis: 牠們每年遷徙兩公里直到數萬年內 , 我們佔據了地球上每一個水域 , 在非常少量的技術下成為稱霸地球的生物 。\n",
      "2022-04-05 02:52:39 | INFO | ml-hw5 | example reference: 其擴展版圖的速度是每年兩公里直到最近幾萬年人已經漫佈到全球各地成為獨霸的生物種類這都是拜小小的科技所賜\n",
      "2022-04-05 02:52:39 | INFO | ml-hw5 | validation loss:\t3.1553\n",
      "2022-04-05 02:52:39 | INFO | ml-hw5 | BLEU = 33.68 62.7/40.2/27.7/20.3 (BP = 0.977 ratio = 0.977 hyp_len = 109238 ref_len = 111811)\n",
      "2022-04-05 02:52:44 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint39.pt\n",
      "2022-04-05 02:52:49 | INFO | ml-hw5 | end of epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:11:47 | INFO | ml-hw5 | training loss: 2.1472\n",
      "2022-04-05 03:11:47 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:12:27 | INFO | ml-hw5 | example source: the first step is attention training .\n",
      "2022-04-05 03:12:27 | INFO | ml-hw5 | example hypothesis: 第一步是專注力訓練 。\n",
      "2022-04-05 03:12:27 | INFO | ml-hw5 | example reference: 第一步是專注力的訓練 。\n",
      "2022-04-05 03:12:27 | INFO | ml-hw5 | validation loss:\t3.1495\n",
      "2022-04-05 03:12:27 | INFO | ml-hw5 | BLEU = 33.35 62.6/40.1/27.5/20.1 (BP = 0.971 ratio = 0.971 hyp_len = 108598 ref_len = 111811)\n",
      "2022-04-05 03:12:32 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint40.pt\n",
      "2022-04-05 03:12:32 | INFO | ml-hw5 | end of epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:31:38 | INFO | ml-hw5 | training loss: 2.1368\n",
      "2022-04-05 03:31:38 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:32:17 | INFO | ml-hw5 | example source: but despite this elaborate decor , sometimes these apartments are used in very unexpected ways , like this home which caught my attention while all the mud and the grass was literally seeping out under the front door .\n",
      "2022-04-05 03:32:17 | INFO | ml-hw5 | example hypothesis: 儘管有這個精緻的裝飾用 , 有時候這些公寓用得非常意想不到 , 像是這個家 , 它吸引了我的注意力 , 所有的泥巴和草幾乎都是在大門之下看見的 。\n",
      "2022-04-05 03:32:17 | INFO | ml-hw5 | example reference: 除了這樣的精心擺設 , 有時候人們會以非常意想不到的方式來使用這些公寓 , 像是這戶人家讓我大開眼界 , 污泥和草幾乎從前門縫底蔓延進來 。\n",
      "2022-04-05 03:32:17 | INFO | ml-hw5 | validation loss:\t3.1533\n",
      "2022-04-05 03:32:17 | INFO | ml-hw5 | BLEU = 33.61 62.6/40.1/27.6/20.1 (BP = 0.979 ratio = 0.979 hyp_len = 109471 ref_len = 111811)\n",
      "2022-04-05 03:32:22 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint41.pt\n",
      "2022-04-05 03:32:22 | INFO | ml-hw5 | end of epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:51:26 | INFO | ml-hw5 | training loss: 2.1272\n",
      "2022-04-05 03:51:26 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 03:52:06 | INFO | ml-hw5 | example source: when i asked one altruist why donating her kidney made sense to her , she said , \" because it's not about me . \"\n",
      "2022-04-05 03:52:06 | INFO | ml-hw5 | example hypothesis: 當我問一位利他主義者 , 為什麼捐出她的腎臟給她覺得合理 , 她說: 「 因為那跟我無關 。 」\n",
      "2022-04-05 03:52:06 | INFO | ml-hw5 | example reference: 當我問一位利他主義者 , 為什麼捐贈腎臟對她來說是正常的 , 她說: 「 因為這不只是關係到我自己 。 」\n",
      "2022-04-05 03:52:06 | INFO | ml-hw5 | validation loss:\t3.1572\n",
      "2022-04-05 03:52:06 | INFO | ml-hw5 | BLEU = 33.83 62.4/40.0/27.6/20.1 (BP = 0.986 ratio = 0.986 hyp_len = 110275 ref_len = 111811)\n",
      "2022-04-05 03:52:12 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint42.pt\n",
      "2022-04-05 03:52:16 | INFO | ml-hw5 | end of epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:11:19 | INFO | ml-hw5 | training loss: 2.1188\n",
      "2022-04-05 04:11:19 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:11:58 | INFO | ml-hw5 | example source: when i asked a woman about medical attention , she laughed , and she said , \" oh , no , no .\n",
      "2022-04-05 04:11:58 | INFO | ml-hw5 | example hypothesis: 當我問一個關於醫療的問題的女人她大笑 , 然後她說: 「 喔 , 不 , 不 。\n",
      "2022-04-05 04:11:58 | INFO | ml-hw5 | example reference: 當我問她們其中一位有否得到醫療協助時 , 她失笑並說: 「 當然沒有 。\n",
      "2022-04-05 04:11:58 | INFO | ml-hw5 | validation loss:\t3.1418\n",
      "2022-04-05 04:11:58 | INFO | ml-hw5 | BLEU = 33.37 62.5/40.1/27.5/20.1 (BP = 0.973 ratio = 0.973 hyp_len = 108810 ref_len = 111811)\n",
      "2022-04-05 04:12:03 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint43.pt\n",
      "2022-04-05 04:12:03 | INFO | ml-hw5 | end of epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:31:10 | INFO | ml-hw5 | training loss: 2.1103\n",
      "2022-04-05 04:31:10 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:31:50 | INFO | ml-hw5 | example source: so for me , everything started many , many years ago when i met the first 3d printer .\n",
      "2022-04-05 04:31:50 | INFO | ml-hw5 | example hypothesis: 對我來說 , 一切都要從好多年前開始 , 那時我遇到了第一台3d列印機 。\n",
      "2022-04-05 04:31:50 | INFO | ml-hw5 | example reference: 對我而言 , 這一切的開始 , 要拉回到好幾好幾年前 , 當我第一次遇上3d印表機時 。\n",
      "2022-04-05 04:31:50 | INFO | ml-hw5 | validation loss:\t3.1434\n",
      "2022-04-05 04:31:50 | INFO | ml-hw5 | BLEU = 33.68 62.7/40.3/27.8/20.4 (BP = 0.974 ratio = 0.974 hyp_len = 108939 ref_len = 111811)\n",
      "2022-04-05 04:31:55 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint44.pt\n",
      "2022-04-05 04:31:55 | INFO | ml-hw5 | end of epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:51:01 | INFO | ml-hw5 | training loss: 2.1009\n",
      "2022-04-05 04:51:01 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 04:51:40 | INFO | ml-hw5 | example source: if we choose to apply our unique capacity to create explanatory knowledge , we could win .\n",
      "2022-04-05 04:51:40 | INFO | ml-hw5 | example hypothesis: 如果我們選擇運用我們獨特的能力來創造解釋性知識 , 我們就有可能贏 。\n",
      "2022-04-05 04:51:40 | INFO | ml-hw5 | example reference: 如果我們選擇用獨特的能力創造解釋性知識 , 我們就有可能贏 。\n",
      "2022-04-05 04:51:40 | INFO | ml-hw5 | validation loss:\t3.1383\n",
      "2022-04-05 04:51:40 | INFO | ml-hw5 | BLEU = 34.03 63.1/40.8/28.3/20.8 (BP = 0.970 ratio = 0.971 hyp_len = 108554 ref_len = 111811)\n",
      "2022-04-05 04:51:45 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint45.pt\n",
      "2022-04-05 04:51:50 | INFO | ml-hw5 | end of epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:10:51 | INFO | ml-hw5 | training loss: 2.0929\n",
      "2022-04-05 05:10:51 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:11:29 | INFO | ml-hw5 | example source: i never saw his face .\n",
      "2022-04-05 05:11:29 | INFO | ml-hw5 | example hypothesis: 我從未見過他的臉 。\n",
      "2022-04-05 05:11:29 | INFO | ml-hw5 | example reference: 我從沒見過他的臉 。\n",
      "2022-04-05 05:11:29 | INFO | ml-hw5 | validation loss:\t3.1415\n",
      "2022-04-05 05:11:29 | INFO | ml-hw5 | BLEU = 33.75 63.1/40.7/28.1/20.7 (BP = 0.965 ratio = 0.966 hyp_len = 108013 ref_len = 111811)\n",
      "2022-04-05 05:11:34 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint46.pt\n",
      "2022-04-05 05:11:34 | INFO | ml-hw5 | end of epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:30:43 | INFO | ml-hw5 | training loss: 2.0850\n",
      "2022-04-05 05:30:43 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:31:23 | INFO | ml-hw5 | example source: i will explain briefly what this is .\n",
      "2022-04-05 05:31:23 | INFO | ml-hw5 | example hypothesis: 我會簡短解釋這是什麼 。\n",
      "2022-04-05 05:31:23 | INFO | ml-hw5 | example reference: 我簡單的說明一下這是甚麼 。\n",
      "2022-04-05 05:31:23 | INFO | ml-hw5 | validation loss:\t3.1342\n",
      "2022-04-05 05:31:23 | INFO | ml-hw5 | BLEU = 33.79 62.8/40.3/27.8/20.4 (BP = 0.976 ratio = 0.977 hyp_len = 109203 ref_len = 111811)\n",
      "2022-04-05 05:31:28 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint47.pt\n",
      "2022-04-05 05:31:28 | INFO | ml-hw5 | end of epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:50:32 | INFO | ml-hw5 | training loss: 2.0771\n",
      "2022-04-05 05:50:32 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 05:51:12 | INFO | ml-hw5 | example source: so he decided he didn't need the opiates anymore , he didn't need the mdma anymore , and he was dropping out of the study .\n",
      "2022-04-05 05:51:12 | INFO | ml-hw5 | example hypothesis: 所以他決定他不再需要鴉片劑了 , 他不再需要mdma了 , 他放棄了研究 。\n",
      "2022-04-05 05:51:12 | INFO | ml-hw5 | example reference: 所以 , 他下定決定 , 他不再需要鴉片劑了 , 他不再需要mdma了 , 他從研究中退出 。\n",
      "2022-04-05 05:51:12 | INFO | ml-hw5 | validation loss:\t3.1270\n",
      "2022-04-05 05:51:12 | INFO | ml-hw5 | BLEU = 34.09 62.4/40.2/27.8/20.5 (BP = 0.985 ratio = 0.985 hyp_len = 110173 ref_len = 111811)\n",
      "2022-04-05 05:51:17 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint48.pt\n",
      "2022-04-05 05:51:22 | INFO | ml-hw5 | end of epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:10:26 | INFO | ml-hw5 | training loss: 2.0696\n",
      "2022-04-05 06:10:26 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:11:05 | INFO | ml-hw5 | example source: no ? yes ? doesn't matter , it worked .\n",
      "2022-04-05 06:11:05 | INFO | ml-hw5 | example hypothesis: 沒有 ? 沒有 ? 沒關係 , 起作用了 。\n",
      "2022-04-05 06:11:05 | INFO | ml-hw5 | example reference: 有 ? 沒有 ? 沒關係 , 反正能用就好 。\n",
      "2022-04-05 06:11:05 | INFO | ml-hw5 | validation loss:\t3.1307\n",
      "2022-04-05 06:11:05 | INFO | ml-hw5 | BLEU = 33.99 63.0/40.7/28.2/20.8 (BP = 0.970 ratio = 0.971 hyp_len = 108537 ref_len = 111811)\n",
      "2022-04-05 06:11:10 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint49.pt\n",
      "2022-04-05 06:11:10 | INFO | ml-hw5 | end of epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:30:13 | INFO | ml-hw5 | training loss: 2.0629\n",
      "2022-04-05 06:30:13 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:30:52 | INFO | ml-hw5 | example source: you're going to read about malware doing incredible and terrifying , scary things .\n",
      "2022-04-05 06:30:52 | INFO | ml-hw5 | example hypothesis: 你們會讀有關惡意軟體的做很了不起、讓人害怕的事 。\n",
      "2022-04-05 06:30:52 | INFO | ml-hw5 | example reference: 你會看到惡意軟體做了很多非常駭人的事情\n",
      "2022-04-05 06:30:52 | INFO | ml-hw5 | validation loss:\t3.1216\n",
      "2022-04-05 06:30:52 | INFO | ml-hw5 | BLEU = 33.90 62.7/40.5/28.1/20.7 (BP = 0.973 ratio = 0.973 hyp_len = 108826 ref_len = 111811)\n",
      "2022-04-05 06:30:57 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint50.pt\n",
      "2022-04-05 06:30:57 | INFO | ml-hw5 | end of epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:50:01 | INFO | ml-hw5 | training loss: 2.0552\n",
      "2022-04-05 06:50:01 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 06:50:41 | INFO | ml-hw5 | example source: he made 15 , 000 dollars cash and bought the house that i grew up in .\n",
      "2022-04-05 06:50:41 | INFO | ml-hw5 | example hypothesis: 他賺了一萬五千美元現金 , 買了我長大的房子 。\n",
      "2022-04-05 06:50:41 | INFO | ml-hw5 | example reference: 他賺了一萬五千美元現金 , 買下了一間房子 , 我就是在這間房子裡長大的 。\n",
      "2022-04-05 06:50:41 | INFO | ml-hw5 | validation loss:\t3.1283\n",
      "2022-04-05 06:50:41 | INFO | ml-hw5 | BLEU = 34.39 62.3/40.1/27.9/20.7 (BP = 0.993 ratio = 0.993 hyp_len = 110986 ref_len = 111811)\n",
      "2022-04-05 06:50:46 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint51.pt\n",
      "2022-04-05 06:50:51 | INFO | ml-hw5 | end of epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:09:49 | INFO | ml-hw5 | training loss: 2.0493\n",
      "2022-04-05 07:09:49 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:10:28 | INFO | ml-hw5 | example source: but in rwanda , in one country , they've got a policy that a woman can't come for care unless she brings the father of the baby with her that's the rule .\n",
      "2022-04-05 07:10:28 | INFO | ml-hw5 | example hypothesis: 但在盧安達 , 在一個國家內 , 她們有一項政策是女人不會前來照顧 , 除非她帶了嬰兒之父和她──那是慣例 。\n",
      "2022-04-05 07:10:28 | INFO | ml-hw5 | example reference: 但在盧安達這地方他們有個政策就是婦女不能單獨尋求醫療照顧一定要孩子的父親陪伴才行這就是規定\n",
      "2022-04-05 07:10:28 | INFO | ml-hw5 | validation loss:\t3.1213\n",
      "2022-04-05 07:10:28 | INFO | ml-hw5 | BLEU = 34.11 62.9/40.7/28.3/21.0 (BP = 0.970 ratio = 0.971 hyp_len = 108540 ref_len = 111811)\n",
      "2022-04-05 07:10:33 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint52.pt\n",
      "2022-04-05 07:10:33 | INFO | ml-hw5 | end of epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:29:34 | INFO | ml-hw5 | training loss: 2.0420\n",
      "2022-04-05 07:29:34 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:30:14 | INFO | ml-hw5 | example source: have no fear for atomic energy 'cause none of them can stop the time .\n",
      "2022-04-05 07:30:14 | INFO | ml-hw5 | example hypothesis: 無懼於原子的能量 , 因為它們沒有一個能停止時間的 。\n",
      "2022-04-05 07:30:14 | INFO | ml-hw5 | example reference: 不懼任何力量 , 因為沒有什麼能停止時間 。\n",
      "2022-04-05 07:30:14 | INFO | ml-hw5 | validation loss:\t3.1199\n",
      "2022-04-05 07:30:14 | INFO | ml-hw5 | BLEU = 34.34 62.7/40.5/28.2/20.9 (BP = 0.981 ratio = 0.982 hyp_len = 109761 ref_len = 111811)\n",
      "2022-04-05 07:30:19 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint53.pt\n",
      "2022-04-05 07:30:19 | INFO | ml-hw5 | end of epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:49:19 | INFO | ml-hw5 | training loss: 2.0364\n",
      "2022-04-05 07:49:19 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 07:49:58 | INFO | ml-hw5 | example source: so let's talk for a moment about race .\n",
      "2022-04-05 07:49:58 | INFO | ml-hw5 | example hypothesis: 我們來談一下種族\n",
      "2022-04-05 07:49:58 | INFO | ml-hw5 | example reference: 我們來談一下種族在美國 , 當我們聽到 「 種族 」 這個單詞\n",
      "2022-04-05 07:49:58 | INFO | ml-hw5 | validation loss:\t3.1231\n",
      "2022-04-05 07:49:58 | INFO | ml-hw5 | BLEU = 34.07 63.2/41.0/28.5/21.0 (BP = 0.966 ratio = 0.966 hyp_len = 108042 ref_len = 111811)\n",
      "2022-04-05 07:50:03 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint54.pt\n",
      "2022-04-05 07:50:03 | INFO | ml-hw5 | end of epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:09:04 | INFO | ml-hw5 | training loss: 2.0303\n",
      "2022-04-05 08:09:04 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:09:43 | INFO | ml-hw5 | example source: they decided that he shouldn't be held indefinitely because he scores high on a checklist that might mean that he would have a greater than average chance of recidivism .\n",
      "2022-04-05 08:09:43 | INFO | ml-hw5 | example hypothesis: 他們決定他不該被無限期地關著 , 因為他高分在一張清單上 , 可能表示他將來會比累犯的平均機會還高 。\n",
      "2022-04-05 08:09:43 | INFO | ml-hw5 | example reference: 他們決定不應該因為他在檢查表中得到高分這高分代表他可能有高於平均值的再犯機率 。 就無限期地關他 。\n",
      "2022-04-05 08:09:43 | INFO | ml-hw5 | validation loss:\t3.1159\n",
      "2022-04-05 08:09:43 | INFO | ml-hw5 | BLEU = 34.39 63.2/41.0/28.5/21.0 (BP = 0.974 ratio = 0.975 hyp_len = 108962 ref_len = 111811)\n",
      "2022-04-05 08:09:48 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint55.pt\n",
      "2022-04-05 08:09:48 | INFO | ml-hw5 | end of epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:28:50 | INFO | ml-hw5 | training loss: 2.0244\n",
      "2022-04-05 08:28:50 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:29:29 | INFO | ml-hw5 | example source: so , if we have a pure wave , we can measure its wavelength , and thus its momentum , but it has no position .\n",
      "2022-04-05 08:29:29 | INFO | ml-hw5 | example hypothesis: 所以 , 如果我們有一個純粹的波浪我們可以測量它的波長進而達到它的動量但是它的動量並沒有偏向\n",
      "2022-04-05 08:29:29 | INFO | ml-hw5 | example reference: 如果我們有一個純粹的波就可以測量它的波長進而算出它的動量但是卻無法測出它的確實位置\n",
      "2022-04-05 08:29:29 | INFO | ml-hw5 | validation loss:\t3.1178\n",
      "2022-04-05 08:29:29 | INFO | ml-hw5 | BLEU = 34.41 63.4/41.2/28.7/21.4 (BP = 0.967 ratio = 0.968 hyp_len = 108187 ref_len = 111811)\n",
      "2022-04-05 08:29:34 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint56.pt\n",
      "2022-04-05 08:29:39 | INFO | ml-hw5 | end of epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:48:41 | INFO | ml-hw5 | training loss: 2.0186\n",
      "2022-04-05 08:48:41 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 08:49:20 | INFO | ml-hw5 | example source: again , see number one .\n",
      "2022-04-05 08:49:20 | INFO | ml-hw5 | example hypothesis: 再次看到第一 ,\n",
      "2022-04-05 08:49:20 | INFO | ml-hw5 | example reference: 同樣的 , 再看一下第一條 。\n",
      "2022-04-05 08:49:20 | INFO | ml-hw5 | validation loss:\t3.1091\n",
      "2022-04-05 08:49:20 | INFO | ml-hw5 | BLEU = 34.49 63.3/41.1/28.7/21.3 (BP = 0.971 ratio = 0.972 hyp_len = 108648 ref_len = 111811)\n",
      "2022-04-05 08:49:25 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint57.pt\n",
      "2022-04-05 08:49:30 | INFO | ml-hw5 | end of epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:08:29 | INFO | ml-hw5 | training loss: 2.0125\n",
      "2022-04-05 09:08:29 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:09:09 | INFO | ml-hw5 | example source: and the permission comes hedged about with qualifiers .\n",
      "2022-04-05 09:09:09 | INFO | ml-hw5 | example hypothesis: 事先同意 , 跟採石器有關 。\n",
      "2022-04-05 09:09:09 | INFO | ml-hw5 | example reference: 而屠殺的許可卻附帶著限制條件\n",
      "2022-04-05 09:09:09 | INFO | ml-hw5 | validation loss:\t3.1068\n",
      "2022-04-05 09:09:09 | INFO | ml-hw5 | BLEU = 34.39 62.9/40.7/28.3/21.0 (BP = 0.979 ratio = 0.980 hyp_len = 109532 ref_len = 111811)\n",
      "2022-04-05 09:09:15 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint58.pt\n",
      "2022-04-05 09:09:15 | INFO | ml-hw5 | end of epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:28:17 | INFO | ml-hw5 | training loss: 2.0081\n",
      "2022-04-05 09:28:17 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:28:57 | INFO | ml-hw5 | example source: if he slept one hour , 30 people would die .\n",
      "2022-04-05 09:28:57 | INFO | ml-hw5 | example hypothesis: 如果他睡一小時 , 30個人會死 。\n",
      "2022-04-05 09:28:57 | INFO | ml-hw5 | example reference: 所以只要他睡一小時 , 就可能有30人死亡\n",
      "2022-04-05 09:28:57 | INFO | ml-hw5 | validation loss:\t3.1053\n",
      "2022-04-05 09:28:57 | INFO | ml-hw5 | BLEU = 34.79 62.8/40.9/28.6/21.3 (BP = 0.983 ratio = 0.983 hyp_len = 109922 ref_len = 111811)\n",
      "2022-04-05 09:29:02 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint59.pt\n",
      "2022-04-05 09:29:07 | INFO | ml-hw5 | end of epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:48:13 | INFO | ml-hw5 | training loss: 2.0028\n",
      "2022-04-05 09:48:13 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:48:52 | INFO | ml-hw5 | example source: if you use apple's keynote , it's got an even better version .\n",
      "2022-04-05 09:48:52 | INFO | ml-hw5 | example hypothesis: 如果你用蘋果的keynote , 它會有一個更棒的版本 。\n",
      "2022-04-05 09:48:52 | INFO | ml-hw5 | example reference: 如果你是使用蘋果的keynote , 就有更好的版本能用\n",
      "2022-04-05 09:48:52 | INFO | ml-hw5 | validation loss:\t3.1049\n",
      "2022-04-05 09:48:52 | INFO | ml-hw5 | BLEU = 34.86 63.0/41.0/28.8/21.4 (BP = 0.981 ratio = 0.981 hyp_len = 109721 ref_len = 111811)\n",
      "2022-04-05 09:48:58 | INFO | ml-hw5 | saved epoch checkpoint: /home/lpz106u/course/ML/hw5/checkpoints/transformer-bt-big/checkpoint60.pt\n",
      "2022-04-05 09:49:02 | INFO | ml-hw5 | end of epoch 60\n"
     ]
    }
   ],
   "source": [
    "if config.run_train:\n",
    "    epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
    "    try_load_checkpoint(model, optimizer, name=config.resume)\n",
    "    while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
    "        # train for one epoch\n",
    "        train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
    "        stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
    "        logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
    "        epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyjRwllxPjtf"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "N70Gc6smPi1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/transformer-bt-big'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/transformer-bt-big/avg_last_5_checkpoint.pt')\n",
      "averaging checkpoints:  ['./checkpoints/transformer-bt-big/checkpoint60.pt', './checkpoints/transformer-bt-big/checkpoint59.pt', './checkpoints/transformer-bt-big/checkpoint58.pt', './checkpoints/transformer-bt-big/checkpoint57.pt', './checkpoints/transformer-bt-big/checkpoint56.pt']\n",
      "Finished writing averaged checkpoint to ./checkpoints/transformer-bt-big/avg_last_5_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# averaging a few checkpoints can have a similar effect to ensemble\n",
    "checkdir=config.savedir\n",
    "!python ./fairseq/scripts/average_checkpoints.py \\\n",
    "--inputs {checkdir} \\\n",
    "--num-epoch-checkpoints 5 \\\n",
    "--output {checkdir}/avg_last_5_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAGMiun8PnZy"
   },
   "source": [
    "## Confirm model weights used to generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "tvRdivVUPnsU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:49:08 | INFO | ml-hw5 | loaded checkpoint checkpoints/transformer-bt-big/avg_last_5_checkpoint.pt: step=unknown loss=3.1049275398254395 bleu=34.85806662987794\n",
      "2022-04-05 09:49:08 | INFO | ml-hw5 | begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:49:49 | INFO | ml-hw5 | example source: pain is something that can make your life miserable .\n",
      "2022-04-05 09:49:49 | INFO | ml-hw5 | example hypothesis: 痛苦是可以讓你的人生悲慘的東西 。\n",
      "2022-04-05 09:49:49 | INFO | ml-hw5 | example reference: 疼痛是一種讓你的生命很不舒服的東西 。\n",
      "2022-04-05 09:49:49 | INFO | ml-hw5 | validation loss:\t3.0739\n",
      "2022-04-05 09:49:49 | INFO | ml-hw5 | BLEU = 35.47 63.5/41.7/29.5/22.1 (BP = 0.978 ratio = 0.978 hyp_len = 109357 ref_len = 111811)\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_last.pt : latest epoch\n",
    "# checkpoint_best.pt : highest validation bleu\n",
    "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
    "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
    "validate(model, task, criterion, log_to_wandb=False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioAIflXpPsxt"
   },
   "source": [
    "## Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "oYMxA8FlPtIq"
   },
   "outputs": [],
   "source": [
    "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
    "    task.load_dataset(split=split, epoch=1)\n",
    "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    idxs = []\n",
    "    hyps = []\n",
    "\n",
    "    model.eval()\n",
    "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(progress):\n",
    "            # validation loss\n",
    "            sample = utils.move_to_cuda(sample, device=device)\n",
    "\n",
    "            # do inference\n",
    "            s, h, r = inference_step(sample, model)\n",
    "            \n",
    "            hyps.extend(h)\n",
    "            idxs.extend(list(sample['id']))\n",
    "            \n",
    "    # sort based on the order before preprocess\n",
    "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
    "    \n",
    "    with open(outfile, \"w\") as f:\n",
    "        for h in hyps:\n",
    "            f.write(h+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Le4RFWXxjmm0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 09:49:49 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: DATA/data-bin/ted2020_with_mono/test.zh-en.en\n",
      "2022-04-05 09:49:49 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: DATA/data-bin/ted2020_with_mono/test.zh-en.zh\n",
      "2022-04-05 09:49:49 | INFO | fairseq.tasks.translation | DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prediction: 100%|██████████| 14/14 [00:30<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_prediction(model, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CZU2beUQtl3"
   },
   "source": [
    "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
    "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
    "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
    "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
    "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
    "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
    "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
    "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
    "9. https://ithelp.ithome.com.tw/articles/10233122\n",
    "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKb4u67-sT_Z",
    "n1rwQysTsdJq",
    "59si_C0Wsms7",
    "oOpG4EBRLwe_",
    "6ZlE_1JnMv56",
    "UDAPmxjRNEEL",
    "ce5n4eS7NQNy",
    "rUB9f1WCNgMH",
    "VFJlkOMONsc6",
    "Gt1lX3DRO_yU",
    "BAGMiun8PnZy",
    "JOVQRHzGQU4-",
    "jegH0bvMQVmR",
    "a65glBVXQZiE",
    "smA0JraEQdxz",
    "Jn4XeawpQjLk"
   ],
   "name": "HW05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
